---
# title: "Let me count the ways to decompose a contingency table"
title: "A large-scale comparison of cross-situational word learning models"
bibliography: refs.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis (kachergis@stanford.edu)} \\ Department of Psychology, 450 Jane Stanford Way \\ Stanford, CA 94305 USA
    \AND {\large \bf Michael C.~Frank (mcfrank@stanford.edu)} \\ Department of Psychology, 450 Jane Stanford Way \\ Stanford, CA 94305 USA}

abstract: >
    How people learn word meanings from ambiguous situations has been a 
    frequent target of empirical work as well as numerous computational models, 
    and yet there is little consensus on the main mechanisms underpinning cross-
    situational word learning. The lack of consensus is in part due to the large 
    number of disparate studies and models, and lack of systematic model comparisons
    across a wide range of studies. This study seeks to remedy this, comparing the 
    performance of several extant cross-situational word learning models on a 
    dataset consisting of 1,696 participants spread across 44 experimental conditions.
    Using cross-validated parameters, we fit multiple models representing theories of both
    associative learning and hypothesis-testing theories of word learning, and discuss
    which models mimic each other, and which mechanisms are overall best able to account 
    for the data. Finally, we test the models' best-fitting parameters ability to generalize
    to a set of additional experiments, including developmental data.
    
keywords: >
    cross-situational word learning; model comparison; language learning
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(grid)
library(ggplot2)
library(xtable)
library(kableExtra)
library(here)
```

# Introduction

Learning word meanings is a crucial aspect of the process of language acquisition. If a learner's task is to connect the words they hear to the potential referents and eventually to generalizable meanings, the challenge appears formidable [@quine1960]: words are heard in rich and cluttered contexts. Yet children learn very quickly. How do they do this? 

An array of social [@yurovsky2017beyond], pragmatic [@Clark1987; @Markman1992], and linguistic [@gleitman1990] cues are often available to reduce a learner's uncertainty. Yet at its heart, these information sources can be seen as reducing the uncertainty inherent in the task of *cross-situational mapping* [@frank2009]: that is, cross-referencing which referents a word has consistently appeared with across multiple uses [@Gleitman1990; @Carey1978]. If learners are able to learn cross-situationally, then they might be able to pool information about reference (and hence about word meaning) over time and learn even in the absence of perfectly disambiguated input in any particular situation. Can children (or even adults) learn in this way? 

The general idea of cross-situational word learning has been adopted into a variety of experimental paradigms for testing the word learning ability of infants [e.g., @Smith2008vg; @Vlach2013], children [e.g., @Akhtar1999; @Suanda2012; @Vlach2012eq], and adults [e.g., @Yu2007; @Smith2010th].
In such experiments, participants are typically presented with a series of training trials comprised of multiple possible referents (e.g., an array of 2-4 novel objects) and one or more spoken nonce words, and (when possible) instructed to learn the meaning(s) of the presented words.
These training trials typically present 10--20 distinct word-referent pairs to be learned across 10--50 trials, presenting each word/referent 3--10 times across a total span of $<10$ minutes.
Learning is then typically assessed by presenting each word and asking learners to make a forced choice of the best referent, either from the entire set or from a small subset of referents.

People of all ages are able to learn word-object mappings in such paradigms at a level significantly above chance, but their performance is typically far from perfect in all but the most trivial versions of the task [@yurovsky2015].
Numerous training manipulations have been found to influence performance. How often mappings are presented  [@Suanda2012;@Kachergis2016], the  interval between appearances of a mapping [@Vlach2012eq; @Vlach2013], and the number of mapppings presented per trial [@Yu2008] all have been shown to affect learning performance.
In sum, the evidence is strong that people can learn word cross-situationally: we are somehow able to integrate evidence (co-occurrence of words and objects) across trials and use that evidence to learn and remember new word meanings in cross-situational learning experiments.
But what are the psychological mechanisms supporting this process? 
And critically, do these mechanisms support naturalistic word learning?

Computational models provide one strategy for instantiating proposals about mechanism. 
Such models can then be applied to naturalistic data to make a first assessment of whether cross-situational learning could be a viable strategy for word learning "in the wild. 
Accordingly, there has been great interest in creating and testing computational models of this task. 
These models often represent distinct intuitions of how learning operates, variously representing the learning problem as logical inferencing [e.g., @Siskind1996], proposing and testing hypotheses [e.g., @Trueswell2013], or  accumulating graded associations [e.g., @Fazly2010; @Kachergis2012gi; @tilles2012minimal]. <!-- or as a blend of the former -->
Extant cross-situational word learning models are too numerous to review exhaustively here. 

It is unknown which of these models -- or even which broad class of models -- best fits the breadth of experimental data. 
While most models of cross-situational learning have typically been tested against at least a few datasets, to our knowledge there has been no systematic comparison of models across a wide variety of experimental conditions. 
Identifying the model or models that best predict human performance is thus an important goal for consolidating this broad literature. 
More broadly, identifying such models could be an important step in assessing the contribution of cross-situational learning to word learning more generally.
<!-- There is even some suggestion that under certain assumptions models representing very different intuitions (e.g., storing a single hypothesized referent for each word vs. accumulating a word x referent association matrix representing all experienced co-occurrences) can mimic each other in some experimental conditions, making some model comparisons ill posed altogether [@Yu2012hyp]. -->

<!-- However, we believe that comparing a large selection of models fitted jointly to a wide variety of experimental conditions may reveal which model(s, classes, mechanism) are most promising for explaining human word learning behavior. -->

Thus, the goal of our current study is to test the ability of a range of cross-situational word learning models in accounting for a wide range of experimental data. 
Our goal is to scale up both the number of models evaluated and the number of experiment designs (and participants) being used for evaluation.
Different models often contain a range of parameters that must be fit to data, an issue that has hindered previous comparison work. 
Thus, we focus on out-of-sample prediction of human data using cross-validated parameter fits.
<!-- , recognizing that the plurality of models and the theories and intuitions they represent can only be winnowed down via rigorous comparison. -->
We end by examining qualititive results from true out of sample generalization of models to experiments not included in the initial evaluation and parameter fitting. 
<!-- Finally, we will interpret the results, and what they entail for theories of cross-situational word learning. -->

# Method

## Models

```{r, model-list}
# need to be run >100 times per given parameter values
stochastic_models = list.files(here("models/stochastic/"))
# assoc models need to be run only once per given parameter values
assoc_models = list.files(here("models/"))
assoc_models = assoc_models[assoc_models!="stochastic"]
```

We fit the following associative models: @Fazly2010, @Kachergis2012gi, ... and a baseline co-occurrence counting model.
We also fit two hypothesis-testing models: guess-and-test [@Medina2011,@Blythe2010] and propose-but-verify [@Trueswell2013].
Finally, we fit two hybrid models, that store multiple, graded hypotheses--but only a subset of all possible associations: pursuit [@Stevens2017pursuit], and a stochastic version of the Kachergis model.
These models and their free parameters are described below.

### Fazly et al. Model
INSERT DESCRIPTION HERE

### Kachergis et al. Model
The @Kachergis2012gi associative model assumes that learners selectively attend more to some of the presented word-object pairings. 
Thus, although all co-occurrences are registered to some extent in associative memory (a word $\times$ object association matrix), greater attention is directed to pairings that have previously co-occurred, resulting in these associations growing more quickly. 
This bias for familiar pairings competes with a bias to attend to stimuli that have no strong associates (e.g., novel stimuli). 
That is, attention is pulled individually to novel stimuli because of the high uncertainty of their associations (i.e., they have diffuse associations with several stimuli). 
Uncertainty is tracked by the entropy of a stimulus' association strengths, and attention is allocated to a stimulus in proportion to this entropy.

Formally, given *n* words and *n* objects to be learned over a series of trials, let \textit{M} be an *n* word $\times$ *n* object association matrix that is built incrementally during training. 
Cell $M_{w,o}$ will be the strength of association between word *w* and object *o*. 
Strengths are augmented by viewing the particular stimuli. 
Before the first trial, \textit{M} is empty. 
On each training trial \textit{t}, a subset \textit{S} of \textit{m} word-object pairings appears. 
If there are any new words and objects are seen, new rows and columns are first added. 
The initial values for these new rows and columns are \textit{k}, a small constant (here, 0.01). 

Association strengths are allowed to decay, and on each new trial a fixed amount of associative weight, $\chi$, is distributed among the associations between words and objects, and added to the strengths. 
The rule used to distribute $\chi$ (i.e., attention) balances a preference for attending to unknown stimuli with a preference for strengthening already-strong associations. 
When a word and referent are repeated, extra attention (i.e., $\chi$) is given to this pair---a bias for prior knowledge. 
Pairs of stimuli with no or weak associates also attract attention, whereas pairings between uncertain objects and known words, or vice-versa, do not attract much attention. 
To capture stimulus uncertainty, strength is allocated using entropy (\textit{H}), a measure of uncertainty that is 0 when the outcome of a variable is certain (e.g., a word appears with one object, and has never appeared with any other object), and maximal ($log_2 n$) when all of the $n$ possible object (or word) associations are equally likely (e.g., when a stimulus has not been observed before, or if a stimulus were to appear with every other stimulus equally). 
In the model, on each trial the entropy of each word (and object) is calculated from the normalized row (column) vector of associations for that word (object), \textit{p}(\textit{M$_w,\cdot$}), as follows:

\begin{equation}
	H(w) = - \sum_{i=1}^{n} p(M_{w,i}) \cdot \textnormal{log}( p(M_{w,i} ) )
	\label{eqn:entropy}
\end{equation}

The update rule for adjusting the association between a given word $w$ and object $o$ on a given trial is:

\begin{equation}
M_{w,o} =  \alpha M_{w,o} + \frac{ \chi \cdot e^{ \lambda \cdot (H(w) + H(o)) } \cdot M_{w,o}  }{  \sum_{w\in W} \sum_{o\in O} e^{ \lambda \cdot (H(w) + H(o)) } \cdot M_{w,o} }
\label{eq:fullmodel}
\end{equation}

In Equation \ref{eq:fullmodel}, $\alpha$ is a parameter governing forgetting, $\chi$ is the weight being distributed, and $\lambda$ is a scaling parameter governing differential weighting of uncertainty ($H(\cdot)$; roughly novelty) and prior knowledge ($M_{w,o}$; familiarity). 
As $\lambda$ increases, the weight of uncertainty (i.e., the exponentiated entropy term, which includes both the word and object's association entropies) increases relative to familiarity. 
The denominator normalizes the numerator so that exactly $\chi$ associative weight is distributed among the potential associations on the trial. 
For stimuli not on a trial, only forgetting operates. 
As each word $w$ is tested, learners choose referent $o$ from the \textit{m} alternatives in proportion to associative strength $M_{w,o}$. 

### Strength-, Uncertainty-, and Novelty-Biased Models
We also separately test the components

### Bayesian Decay Model

### Tilles Model

### Rescorla-Wagner Model


### Propose-but-verify Model
In the propose-but-verify hypothesis testing model @Trueswell2013, one of the presented referents is selected at random for any word heard that has no remembered referent. 
The next time that word occurs, the previously-proposed referent is remembered with probability $\alpha$, a free parameter. 
If the remembered referent is verified to be present, the future probability of recalling the word is increased by $\epsilon$ (another free parameter). 
If the remembered referent is not present, the old hypothesis is assumed to be forgotten and a new proposal is selected from the available referents.
This model implements trial-level mutual exclusivity by selecting new proposals only from among the referents that are not yet part of a hypothesis.

### Guess-and-test Model
The guess-and-test hypothesis testing model is based on the description given by @Medina2011 of a one-shot (i.e. "fast mapping") learning, which posits that "i) learners hypothesize a single meaning based on their first encounter with a word, ii) learners neither weight nor even store back-up alternative meanings, and iii) on later encounters, learners attempt to retrieve this hypothesis from memory and test it against a new context, updating it only if it is disconfirmed."
In summary, guess-and-test learners do *not* reach a final hypothesis by comparing multiple episodic memories of prior contexts or multiple semantic hypotheses.
We give this model two free parameters: a probability of successful encoding ($s$, hypothesis formation), and a probability $f$ of forgetting a hypothesis at retrieval. 
This model is quite similar to the guess-and-test model formally analyzed by @Blythe2010 to determine the theoretical long-term efficacy of cross-situational word learning.

## Data

```{r load-data}
load(here("data/combined_data.RData"))

tab <- tibble("Name"=NA, "Trials"=NA, "Words/Tr"=NA, "Objs/Tr"=NA, "Items"=NA, "Accuracy"=NA, "SD"=NA, "N"=NA) 

totSs = 0
totItems = 0
for(c in 1:length(combined_data)) { 
  totSs = totSs + combined_data[[c]]$Nsubj 
  totItems = totItems + length(combined_data[[c]]$HumanItemAcc)
  tab <- bind_rows(tab, c("Name" = combined_data[[c]]$Condition, 
                          "Trials" = nrow(combined_data[[c]]$train$words), # trials
                          "Words/Tr" = ncol(combined_data[[c]]$train$words), # words
                          "Objs/Tr" = ncol(combined_data[[c]]$train$objs), # objs
                          "Items" =length(combined_data[[c]]$HumanItemAcc),
                          "Accuracy" = mean(combined_data[[c]]$HumanItemAcc),
                          "SD" = sd(combined_data[[c]]$HumanItemAcc),
                          "N" = combined_data[[c]]$Nsubj
                          ))
}

tab = na.omit(tab)
tab <- tab %>% mutate(Accuracy = as.numeric(tab$Accuracy),
               SD = as.numeric(SD))
```

The modeled data are average accuracies from `r totItems` word-object pairs in `r length(combined_data)` experimental conditions, in which a total of `r totSs` subjects participated.
Table 2 in the Appendix shows a variety of characteristics of each experimental condition, including the number of trials, the number of words and referents presented per training trial, the number of total to-be-learned pairs, the number of participants per condition, and the average accuracy and SD of performance.
Most of these data have been previously published: data from 13 of the conditions were reported in @Kachergis2013nat, data from four conditions are from @Kachergis2012gi, and data from another four conditions are from @Kachergis2016.
However, data from several conditions have never been published, but were shared Chen Yu's lab at Indiana University.

Each condition consists of an ordered list of training trials consisting of 1-4 words and 2-4 objects per trial.


## Procedure

We will first optimize each model's parameters with 5-fold cross-validation, leaving out data from 20% of the conditions to test generalization from optimizing to the remaining 80%.
We will then optimize each model's parameterss to the entire dataset, and use these parameters to test generalization to a selection of other experiments from the literature for which we do not have item-level accuracy.

## Results

```{r load-cv-fits}

load(here("fits/cv_group_fits.Rdata"))
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg_fit_tab <- cvg %>% group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)
```


```{r cv-fits-table, results="asis"}
# how to do 2-col table? , fig.env = "figure*" 
tab2 <- xtable::xtable(cvg_fit_tab, digits=3, 
                       caption = "Cross-validated group fits.")

print(tab2, type="latex", comment = F, include.rownames = F, table.placement = "H")
```


```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=6, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Human vs. model performance with a single best-fitting set of parameters for all conditions."}

# each model jointly fit to all conditions
load(here("fits/group_fits.Rdata"))
#for(m in names(group_fits)) { 
#  print(paste(m, round(group_fits[[m]]$optim$bestval, 2)))
#}
# each model separately fit to each condition (should try cross-validation approach?)

#load(here("fits/cond_fits.Rdata"))

gcondm <- gfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#gcondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)

#gcondm %>%
#  ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition, size=Nsubj)) + 
#  geom_point(alpha=.7) + facet_wrap(vars(Model)) + theme_bw() +
#  geom_abline(intercept=0, slope=1, linetype="dashed")

gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, 
                   group=Condition, color=Condition)) + 
  geom_point(alpha=.5) + 
  facet_wrap(vars(Model)) + 
  theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed") + 
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 6), 
        legend.spacing = unit(.5, "points"))
```



# Discussion



# Acknowledgements

Thanks to Chen Yu for allowing us to include data from several previously-unpublished experimental conditions.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

<div id="refs"></div>

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

```{r, child = "appendix.Rmd"}
```
