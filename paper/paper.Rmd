---
# title: "Let me count the ways to decompose a contingency table"
title: "A large-scale comparison of cross-situational word learning models"
bibliography: refs.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis (kachergis@stanford.edu)} \and {\large \bf Michael C.~Frank (mcfrank@stanford.edu)} \\ Department of Psychology, 450 Jane Stanford Way \\ Stanford, CA 94305 USA}

abstract: >
    How people learn word meanings from ambiguous situations has been a 
    frequent target of empirical work as well as numerous computational models, 
    and yet there is little consensus on the main mechanisms underpinning cross-
    situational word learning. The lack of consensus is in part due to the large 
    number of disparate studies and models, and lack of systematic model comparisons
    across a wide range of studies. This study seeks to remedy this, comparing the 
    performance of several extant cross-situational word learning models on a 
    dataset consisting of 1,696 participants spread across 44 experimental conditions.
    Using cross-validated parameters, we fit multiple models representing theories of both
    associative learning and hypothesis-testing theories of word learning, and discuss
    which models mimic each other, and which mechanisms are overall best able to account 
    for the data. 
    
keywords: >
    cross-situational word learning; model comparison; language learning
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
# Finally, we test the models' best-fitting parameters ability to generalize to a set of additional experiments, including developmental data.
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(grid)
library(ggplot2)
library(xtable)
library(kableExtra)
library(here)
```

# Introduction

Learning word meanings is a crucial aspect of the process of language acquisition. 
If a learner's task is to connect the words they hear to the potential referents and eventually to generalizable meanings, the challenge appears formidable [@Quine1960]: words are heard in rich and cluttered contexts. 
Yet children learn words very quickly. How do they do this? 

An array of social [@yurovsky2017beyond], pragmatic [@Clark1987; @Markman1992], and linguistic [@Gleitman1990] cues are often available to reduce a learner's uncertainty. 
Yet at its heart, these information sources can be seen as reducing the uncertainty inherent in the task of *cross-situational mapping* [@Frank2009]: that is, cross-referencing which referents a word has consistently appeared with across multiple uses [@Gleitman1990; @Carey1978]. 
If learners are able to learn cross-situationally, then they might be able to pool information about reference (and hence about word meaning) over time and learn even in the absence of perfectly disambiguated input in any particular situation. 
Can children (or even adults) learn in this way? 

The general idea of cross-situational word learning has been adopted into a variety of experimental paradigms for testing the word learning ability of infants [e.g., @Smith2008vg; @Vlach2013], children [e.g., @Akhtar1999; @Suanda2012; @Vlach2012eq], and adults [e.g., @Yu2007; @Smith2010th].
In such experiments, participants are typically presented with a series of training trials comprised of multiple possible referents (e.g., an array of 2-4 novel objects) and one or more spoken nonce words, and (when possible) instructed to learn the meaning(s) of the presented words.
These training trials typically present 10--20 distinct word-referent pairs to be learned across 10--50 trials, presenting each word/referent 3--10 times across a total span of $<10$ minutes.
Learning is then typically assessed by presenting each word and asking learners to make a forced choice of the best referent, either from the entire set or from a small subset of referents.

People of all ages are able to learn word-object mappings in such paradigms at a level significantly above chance, but their performance is typically far from perfect in all but the most trivial versions of the task [@yurovsky2017beyond].
Numerous training manipulations have been found to influence performance. 
How often mappings are presented  [@Suanda2012; @Kachergis2016], the  interval between appearances of a mapping [@Vlach2012eq; @Vlach2013], and the number of mappings presented per trial [@Yu2008] all have been shown to affect learning performance.
In sum, the evidence is strong that people can learn word cross-situationally: we are somehow able to integrate evidence (co-occurrence of words and objects) across trials and use that evidence to learn and remember new word meanings in cross-situational learning experiments.
But what are the psychological mechanisms supporting this process? 
And critically, do these mechanisms support naturalistic word learning?

Computational models provide one strategy for instantiating proposals about mechanism. 
Such models can then be applied to naturalistic data to make a first assessment of whether cross-situational learning could be a viable strategy for word learning "in the wild". 
Accordingly, there has been great interest in creating and testing computational models of this task. 
These models often represent distinct intuitions of how learning operates, variously representing the learning problem as logical inferencing [e.g., @Siskind1996], proposing and testing hypotheses [e.g., @Trueswell2013], or accumulating graded associations [e.g., @Fazly2010; @Kachergis2012gi; @tilles2012minimal]. <!-- or as a blend of the former -->
Extant cross-situational word learning models are too numerous to review exhaustively here. 

It is unknown which of these models -- or even which broad class of models -- best fits the breadth of experimental data. 
While most models of cross-situational learning have typically been tested against at least a few datasets, to our knowledge there has been no systematic comparison of models across a wide variety of experimental conditions. 
Identifying the model or models that best predict human performance is thus an important goal for consolidating this broad literature. 
More broadly, identifying such models could be an important step in assessing the contribution of cross-situational learning to word learning more generally.

<!-- However, we believe that comparing a large selection of models fitted jointly to a wide variety of experimental conditions may reveal which model(s, classes, mechanism) are most promising for explaining human word learning behavior. -->

Thus, the goal of our current study is to test the ability of a range of cross-situational word learning models in accounting for a wide range of experimental data. 
Our goal is to scale up both the number of models evaluated and the number of experiment designs (and participants) being used for evaluation.
Different models often contain a range of parameters that must be fit to data, an issue that has hindered previous comparison work. 
Thus, we focus on out-of-sample prediction of human data using cross-validated parameter fits.
<!-- , recognizing that the plurality of models and the theories and intuitions they represent can only be winnowed down via rigorous comparison. -->
<!-- We end by examining qualitative results from true out of sample generalization of models to experiments not included in the initial evaluation and parameter fitting.  -->
Finally, we will interpret the results, and what they entail for theories of cross-situational word learning.

# Method
\vspace{-6pt}
## Procedure
\vspace{-6pt}
In a nutshell, our goal is to fit a set of models to a set of datasets; members of each of these sets are listed below.
Each model typically is defined as a function that takes input data and a group of parameter values (often between 2--4) and then can be evaluated on test trials (typically multi-alternative forced choice test). 
Each model then returns predictions on each test trial, which can be scored against the "correct" (intended by the design) answer and averaged to produce an analogue to human performance. 
Our primary outcome measure is the correlation between each model's performance for each item in a specific experimental condition and item-level human performance on that same experimental condition. 

Although input data and test trials are features of specific experimental tests, each  model has parameters that must be specified. 
Typically the settings of these parameters are critical to the predictive performance of the model. 
Hence, to ensure a fair comparison between models, these parameters must be fit to data. 
In all cases, we optimize parameter values to minimize the sum of squared error (SSE) between model and human performance, weighting the contribution of each condition's SSE by the number of participants in that condition.
We optimize parameters using differential evolution via the DEoptim R package [@DEoptim].

We conducted two evaluations of models. 
First, we use cross-validation across conditions to reduce overfitting. 
We optimize each model's parameters by leaving out data from 20% of conditions to test generalization via optimization on the initial 80% (5-fold cross-validation).
Second, in the *group fit* regime we fit each model to all conditions and datasets to produce a single best-fitting parameter set per model. 
We then evaluated the correlation between these fitted models and the same data they were fit to. 
This evaluation thus makes maximal use of our data but incorporates some amount of overfitting.
However, it allows us to make the best test of generalization to a selection of other experiments from the literature for which we do not have item-level accuracy.

## Models
\vspace{-8pt}
```{r, model-list}
# need to be run >100 times per given parameter values
stochastic_models = list.files(here("models/stochastic/"))
# assoc models need to be run only once per given parameter values
assoc_models = list.files(here("models/"))
assoc_models = assoc_models[assoc_models!="stochastic"]
```
We fit the following associative models: @Fazly2010, @Kachergis2012gi, three simplified variations of that model, a Bayesian decay model, and a baseline co-occurrence counting model.
We also fit two hypothesis-testing models: guess-and-test [@Medina2011; @Blythe2010] and propose-but-verify [@Trueswell2013].
Finally, we fit two hybrid models, that store multiple, graded hypotheses--but only a subset of all possible associations: pursuit [@Stevens2017pursuit], and a stochastic version of the Kachergis model.
These models and their free parameters are briefly described below.

### Baseline Co-occurrence Model
The baseline model simply tallies the co-occurrences of any words and objects appearing together on each trial, accumulating the counts in a word $\times$ object matrix.
<!-- tried adding a softmax parameter but it didn't help --> 

### Fazly et al. Model
The @Fazly2010 model represents the meaning of each word *w* as a probability distribution $p(\cdot | w)$ over the objects appearing across the trials, and incrementally learns these distributions trial-by-trial.
The association between a given *w* and *o* grows more quickly if $p(w|o)$ is already high (a familiarity bias), unless some other word has a strong association with *o*; thus, associations are competitive.
A parameter $\lambda$ is a small smoothing constant added to both the numerator and denominator of each $p(w|o)$, further adjusted in the denominator by a $\beta$ parameter representing the upper bound on the number of symbol types.

### Kachergis et al. Model
The @Kachergis2012gi associative model assumes that learners associate all presented words with all visible objects to some extent, but that they selectively attend more to some of the presented word-object pairings. 
Specifically, greater attention is directed to pairings that have previously co-occurred (a familiarity bias), but is also directed to stimuli that have no strong associates, tracked via the entropy (*H*) of their association strengths (e.g., novel stimuli, or stimuli that have diffuse associations with several stimuli). 
Thus, the model grows its association matrix as it experiences each trial, dynamically apportioning a fixed amount of associative weight (learning rate $\chi$) among the possible word-object associations, with the relative weight of the familiarity bias and the uncertainty bias determined by parameter $\lambda$.
Finally, all association strengths decay from trial to trial at a rate controlled by parameter $\alpha$.

The update rule for adjusting the association $M_{w,o}$ between a given word $w$ and object $o$ on a given trial is:
\vspace{-6pt}
\begin{equation}
M_{w,o} =  \alpha M_{w,o} + \frac{ \chi \cdot e^{ \lambda \cdot (H(w) + H(o)) } \cdot M_{w,o}  }{  \sum_{w\in W} \sum_{o\in O} e^{ \lambda \cdot (H(w) + H(o)) } \cdot M_{w,o} }
\label{eq:fullmodel}
\end{equation}

At test, given word $w$ learners choose referent $o$ from the \textit{m} given alternatives in proportion to associative strength $M_{w,o}$ (i.e. Luce's choice rule). 

We also fit a stochastic *sampling* version of this model [see @Kachergis2017], which uses the same update equation, but instead of updating all possible word-object associations samples only a single presented object for each word on a trial. 
This sampling version of the Kachergis model can still build multiple graded associations for each word (or object), but on any given run will accumulate only a sparse, randomized version of what the full associative model would build.

### Strength-, Uncertainty-, and Novelty-Biased Models
We also test three variations on the Kachergis et al. model that use only a subset of the mechanisms of the full model in order to understand the contributions of each mechanism.
The *strength model* lacks the uncertainty bias term, and thus only implements a bias for familiar (i.e., already-strong) associations, with learning rate ($\chi$) and decay $\alpha$ parameters.

The *uncertainty model* lacks the strength bias term, and thus only implements a bias for stimuli with uncertain (high-entropy) associations.
The *novelty model* lacks the strength bias term, and substitutes novelty for the entropy terms (e.g., 1/(frequency(*w*)+1) instead of $H(w)$.
These models still have all three parameters of the original Kachergis model. <!--, although $\chi$ and $\beta$ likely trade off.-->

### Bayesian Decay Model
This previously-unpublished model updates the $p(w|o)$ and the joint probability $p(w,o)$ from trial to trial according to a likelihood function that reinforces the association between *w* and *o* when they co-occur (scaled by parameter $\delta$), and penalizes all associations that are not occurring on the trial (scaled by parameter $\alpha$).
Thus, in contrast to other incremental associative learning models considered here (e.g., Fazly and the Kachergis class of models), this model globally updates the entire association matrix on each trial -- both co-occurring pairs and non-co-occurring pairs, and re-normalizes the tracked probabilities at each time step.
At test, this model uses a softmax choice rule with parameter $\tau$. 

<!-- ### Tilles Model -->

### Rescorla-Wagner Model
This associative model is inspired by the prediction-error-based learning model of @Rescorla1972. 
Objects on each trial serve as cues to predict which words will be heard.
When a given word is heard, the associations between that word and each object on the trial are scaled at a learning rate $\beta$ in proportion to the magnitude of the difference between the prediction and the maximum association value ($\lambda$). 
Each trial, the association matrix is subject to decay (parameter $\alpha$).

### Propose-but-verify Model
In the propose-but-verify hypothesis testing model [@Trueswell2013], one of the presented referents is selected at random for any word heard that has no remembered referent. 
The next time that word occurs, the previously-proposed referent is remembered with probability $\alpha$, a free parameter. 
If the remembered referent is verified to be present, the future probability of recalling the word is increased by $\epsilon$ (another free parameter). 
If the remembered referent is not present, the old hypothesis is assumed to be forgotten and a new proposal is selected from the available referents.
This model implements trial-level mutual exclusivity by selecting new proposals only from among the referents that are not yet part of a hypothesis.

### Guess-and-test Model
The guess-and-test hypothesis testing model is based on the description given by @Medina2011 of a one-shot (i.e. "fast mapping") learning, which posits that "i) learners hypothesize a single meaning based on their first encounter with a word, ii) learners neither weight nor even store back-up alternative meanings, and iii) on later encounters, learners attempt to retrieve this hypothesis from memory and test it against a new context, updating it only if it is disconfirmed."
In summary, guess-and-test learners do *not* reach a final hypothesis by comparing multiple episodic memories of prior contexts or multiple semantic hypotheses.
We give this model two free parameters: a probability of successful encoding ($s$, hypothesis formation), and a probability $f$ of forgetting a hypothesis at retrieval. 
This model is quite similar to the guess-and-test model formally analyzed by @Blythe2010 to determine the theoretical long-term efficacy of cross-situational word learning.


### Pursuit Model
The pursuit model [@Stevens2017pursuit] represents a hybrid approach, potentially storing graded associations involving a given word and multiple referents (or vice-versa), while on a given trial greedily pursuing the single strongest association for each presented word.
If the strongest associated referent for a given word $w$ is present on the trial, the association is strengthened at a rate determined by $\gamma$.
If the strongest referent for $w$ is not present, the association is weakened (scaled down by ($1-\gamma$)) and the association with a random other available referent is strengthened.
Novel words are given an initial association of strength $\gamma$ with the available referent whose strongest association is the smallest, implementing an uncertainty bias in forming associations for novel words.
A given word-object association only enters the lexicon if $p(o|w) > \theta$, a threshold parameter.

## Data
\vspace{-9pt}
```{r load-data}
load(here("data/combined_data.RData"))

tab <- tibble("Name"=NA, "Trials"=NA, "W/Tr"=NA, "O/Tr"=NA, "Items"=NA, "Acc"=NA, "SD"=NA, "N"=NA) 

totSs = 0
totItems = 0
for(c in 1:length(combined_data)) { 
  totSs = totSs + combined_data[[c]]$Nsubj 
  totItems = totItems + length(combined_data[[c]]$HumanItemAcc)
  tab <- bind_rows(tab, c("Name" = combined_data[[c]]$Condition, 
                          "Trials" = nrow(combined_data[[c]]$train$words), # trials
                          "W/Tr" = ncol(combined_data[[c]]$train$words), # words
                          "O/Tr" = ncol(combined_data[[c]]$train$objs), # objs
                          "Items" =length(combined_data[[c]]$HumanItemAcc),
                          "Acc" = mean(combined_data[[c]]$HumanItemAcc),
                          "SD" = sd(combined_data[[c]]$HumanItemAcc),
                          "N" = combined_data[[c]]$Nsubj
                          ))
}

tab = na.omit(tab)
tab <- tab %>% mutate(Acc = as.numeric(tab$Acc),
               SD = as.numeric(SD))
```

The modeled data are average accuracies from `r totItems` word-object pairs in `r length(combined_data)` experimental conditions, in which a total of `r totSs` subjects participated.
Table 3 shows a variety of characteristics of each experimental condition, including the number of trials (range: 18-108), the number of words and referents presented per training trial (1-4), the number of total to-be-learned pairs (12-24), the number of participants per condition, and the average accuracy and SD of performance.
Most of these data have been previously published: data from 13 of the conditions were reported in @Kachergis2013nat, data from 12 conditions are from @Kachergis2012gi, and data from another nine conditions are from @Kachergis2016.
However, data from several conditions run in Chen Yu's lab at Indiana University have never before been published.
Each condition consists of an ordered list of training trials consisting of 1-4 words and 2-4 objects per trial.

# Results

```{r load-group-fits}
load(here("fits/group_fits.Rdata"))
Nitems = nrow(gfd) / length(unique(gfd$Model)) # MSE per item? per condition?
group_fit_tab <- gfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2), #/ Nitems,
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)
```


```{r load-cv-fits}

load(here("fits/cv_group_fits.Rdata"))
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg_fit_tab <- cvg %>% group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2), #  / Nitems,
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE) %>%
  rename(`CV SSE` = SSE,
         ` r` = r) %>% left_join(group_fit_tab) %>%
  rename(`All SSE` = SSE) 

cvg_fit_tab$Model = c("Kachergis samp", "Fazly", "Kachergis", "Novelty", "Uncertainty", "Bayes decay", "Rescorla-Wagner", "Strength", "Propose-Verify", "Guess-Test", "Pursuit")

cvg_fit_tab$P = c(3, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3)
```

Table 1 shows the sum of squared error (SSE) and correlation ($r$) for each model's best-fitting parameters vs. average human performance on the `r totItems` items, both for the CV fits and for the all-condition fits, along with the number of fitted parameters in each model (P).
The results of the two fitting procedures yielded fairly similar SSEs and correlations, and a consistent rank-ordering of the models, with the Kachergis sampling model performing best, followed closely by the Fazly et al. model and then the associative Kachergis et al. model.
The hypothesis testing models (Propose-but-Verify and Guess-and-Test) and the Pursuit model fit less well than the 0-parameter baseline co-occurrence counting model (not in the table), which had SSE = 32.0 and $r = 0.53$.
Correlations between data and models are presented in Figure 1. 

```{r model-correlations}
# remove leading 0. to make table fit in 1 column
numformat <- function(val) { sub("^(-?)0.", "\\1.", sprintf("%.2f", val)) }

mod_names <- group_fit_tab$Model
mod_cors <- matrix('-', nrow=length(mod_names), ncol=length(mod_names))
for(r in 1:nrow(mod_cors)) {
  for(c in 1:ncol(mod_cors)) {
    if(r>c) mod_cors[r,c] = numformat(round(cor(subset(cvg, Model==mod_names[r])$ModelPerf,
                                subset(cvg, Model==mod_names[c])$ModelPerf), 2))
  }
}
mod_names_short = c("Ks","F","K","N","U","B","R","S","V","G","P")
rownames(mod_cors) = mod_names_short
colnames(mod_cors) = mod_names_short
```

How related to each other are these models?
Table 2 shows the correlations between model predictions made using the group-optimized parameters, with the names of the models abbreviated for space (*Ks*: Kachergis sampling, *F*azly, *K*achergis, *N*ovelty, *U*ncertainty, *B*ayesian decay, *R*escorla-Wagner, *S*trength, Propose-but-*V*erify, *G*uess-and-Test, *P*ursuit).
These results are discussed below.

```{r cv-fits-table, results="asis"}
# how to do 2-col table? , fig.env = "figure*" 
tab2 <- xtable::xtable(cvg_fit_tab, digits=c(0,0, 2, 2, 2, 2, 0), 
                       caption = "Cross-validated and group model fits.")

print(tab2, type="latex", comment = F, include.rownames = F, table.placement = "H")
```

```{r model-cors-table, results="asis"}
tab3 <- xtable::xtable(mod_cors[2:nrow(mod_cors),1:(ncol(mod_cors)-1)],  caption = "Correlations of models' predictions.")

print(tab3, type="latex", comment = F, include.rownames = T, table.placement = "H")
```

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=6.0, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Human vs. model performance with a single best-fitting set of parameters for all conditions."}

gcondm <- gfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#gcondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)

#gcondm %>%
#  ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition, size=Nsubj)) + 
#  geom_point(alpha=.7) + facet_wrap(vars(Model)) + theme_bw() +
#  geom_abline(intercept=0, slope=1, linetype="dashed")

gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, 
                   group=Condition, color=Condition)) + 
  geom_point(alpha=.3) + 
  facet_wrap(vars(Model)) + 
  theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed") + 
  theme(#legend.title = element_blank(),
        legend.margin = margin(0, 0, 0, 0, "cm"),
        legend.box.margin = margin(0, 0, 0, 0, "cm"), 
        legend.box.spacing = unit(0.0, "cm"),
        legend.position = "bottom", 
        legend.text = element_text(size = 7, margin = margin(t=2)), 
        legend.spacing.x = unit(.1, 'cm'),
        legend.spacing.y = unit(0, 'cm'))
```

# Discussion
We set out to conduct a systematic comparison of several models of cross-situational word learning models across a wide variety of experimental conditions. 
Using a database of `r totSs` participants in `r length(combined_data)` conditions, we found best-fitting parameters for 11 different models under two different regimes: 1) simultaneously fitting all of the data, and 2) 5-fold cross-validation, testing generalization to the final 20% of conditions after training on 80% of them.
The results of both fitting regimes were consistent and clear: shown in Table 1, the associative models achieved much better fits (lower SSE and higher correlation) than the hypothesis-testing models (Guess-and-Test; @Medina2011 and Propose-but-Verify; @Trueswell2013), which made strongly-correlated predictions (see Table 2: .95).
The hybrid Pursuit model [@Stevens2017pursuit] fared even more poorly, in part because it often outperformed humans ($M_{mod} =$ `r round(mean(subset(gfd, Model=="pursuit_detailed")$ModelPerf), 2)`), $M_{hum} = 0.48$).

Among the associative models, the @Fazly2010 model and the @Kachergis2012gi model both fare quite well, and the sampling version of the Kachergis et al. model performs slightly better, albeit with 3 free parameters compared to 2 in the Fazly et al. model.
The predictions of the top three models are more correlated with each other than with the human data (see Table 2): Fazly vs. Kachergis sampling: .87, Fazly vs. Kachergis: .84, Kachergis vs. sampling: .92.
But the Fazly model also showed a .84 correlation with the Rescorla-Wagner model, and both Kachergis models showed a .87 correlation with the simpler novelty-biased associative model--stronger than for the uncertainty-bias mechanism used by these models.


```{r models-diverge}
mw <- gfd %>% mutate(ID = rep(1:726, 11)) %>%
  pivot_wider(names_from=Model, values_from=ModelPerf)

divs <- mw %>% select(condnum, Condition, kachergis, fazly, kachergis_sampling) %>%
  mutate(kf_div = (kachergis-fazly)^2,
         ksf_div = (kachergis_sampling-fazly)^2,
         ksk_div = (kachergis_sampling-kachergis)^2) %>%
  group_by(Condition) %>%
  summarise(kf_div = sum(kf_div),
            ksf_div = sum(ksf_div),
            ksk_div = sum(ksk_div)) %>%
  arrange(desc(kf_div), desc(ksk_div))
# maximally divergent in 1x3 - combined_data[[12]]
# and 1x3 6/18 4AFC - combined_data[[8]]


#mean(subset(gfd, Model=="fazly" & Condition=="1x3")$ModelPerf) # .34
#mean(subset(gfd, Model=="kachergis" & Condition=="1x3")$ModelPerf) # .48
#mean(subset(gfd, Model=="fazly" & Condition=="1x3")$HumanPerf) # 57

#mean(subset(gfd, Model=="fazly" & Condition=="1x4")$ModelPerf) # .27
#mean(subset(gfd, Model=="kachergis" & Condition=="1x4")$ModelPerf) # .40
#mean(subset(gfd, Model=="fazly" & Condition=="1x4")$HumanPerf) # .19
```

The Kachergis and Fazly models make maximally divergent predictions in the conditions with fewer words than objects per trial (esp. 1x3, 1x3 6/18 4AFC, and 1x4), which hints at a difference in mechanism for these models and may guide the design of a new experiment to diagnose which is better able to account for human behavior.

In summary, we have shown not only how a large model comparison can help rank models from most to least able to account for human data, but also how such comparisons can identify models that mimic each other, as well as identifying experimental designs that might aid in distinguishing models.
Future work should extend this comparison to models and datasets beyond those tested here, and in particular to developmental experiments in order to test the models' generality for explaining word learning across a lifetime.

<!-- In three cross-situational learning experiments, Hendrickson gave participants word-object pairs occurring with Zipf (power-law) frequency distribution, and then compared two models:  -->

<!-- There is even some suggestion that under certain assumptions models representing very different intuitions (e.g., storing a single hypothesized referent for each word vs. accumulating a word x referent association matrix representing all experienced co-occurrences) can mimic each other in some experimental conditions, making some model comparisons ill posed altogether [@Yu2012hyp]. -->


# Acknowledgements

We thank Chen Yu for allowing us to include data from several previously-unpublished experimental conditions.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

<div id="refs"></div>

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

```{r, child = "appendix.Rmd"}
```
