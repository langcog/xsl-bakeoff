---
# title: "Let me count the ways to decompose a contingency table"
title: "A large-scale comparison of several cross-situational word learning models"
bibliography: refs.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis (kachergis@stanford.edu)} \\ Department of Psychology, 450 Jane Stanford Way \\ Stanford, CA 94305 USA
    \AND {\large \bf Michael C.~Frank (mcfrank@stanford.edu)} \\ Department of Psychology, 450 Jane Stanford Way \\ Stanford, CA 94305 USA}

abstract: >
    How people learn word meanings from ambiguous situations has been a 
    frequent target of empirical work as well as numerous computational models, 
    and yet there is little consensus on the main mechanisms underpinning cross-
    situational word learning. The lack of consensus is in part due to the large 
    number of disparate studies and models, and lack of systematic model comparisons
    across a wide range of studies. This study seeks to remedy this, comparing the 
    performance of several extant cross-situational word learning models on a 
    dataset consisting of 1,696 participants spread across 44 experimental conditions.
    Using cross-validated parameters, we fit multiple models representing theories of both
    associative learning and hypothesis-testing theories of word learning, and discuss
    which models mimic each other, and which mechanisms are overall best able to account 
    for the data. Finally, we test the models' best-fitting parameters ability to generalize
    to a set of additional experiments, including developmental data.
    
keywords: >
    cross-situational word learning; model comparison; language learning
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(grid)
library(ggplot2)
library(xtable)
library(kableExtra)
library(here)
```

# Introduction

Learning word meanings is a crucial aspect of mastering a language that is theorized to be quite difficult, since learners often face a high degree of ambiguity about what aspect(s) of a scene (if any) each uttered word refers to.
And yet the vocabularies of young children grow surprisingly quickly, inviting the question of how their still-developing cognitive processes are able to support such rapid word learning with apparent ease and robustness.
While an array of social [@yurovsky2017beyond], pragmatic [@Clark1987; Markman1992], and linguistic [/other? @Merriman1989; CITE] cues are often available to direct attention which may reduce a learner's uncertainty, language learning researchers have pointed out that the difficulty of the word learning problem might dissolve if learners are able to *somehow* cross-reference which referents each word has consistently appeared with across multiple situations (i.e., cross-situationally; @Gleitman1990; @Carey1978), as the learner may then be able to correctly map the word to its highest-frequency referent.
This general idea of cross-situational word learning has been adopted into a variety of experimental paradigms for testing the word learning ability of infants [e.g., @Smith2008vg; @Vlach2013], children [e.g., @Akhtar1999; @Suanda2012; @Vlach2012eq], and adults' [e.g., @Yu2007; @Smith2010th].
In such experiments, participants are typically presented with a series of training trials comprised of multiple possible referents (e.g., an array of 2-4 novel objects) and one or more spoken nonce words, and (when possible) instructed to learn the meaning(s) of the presented words.
These training trials typically present 10-20 distinct word-referent pairs to be learned across 10-50 trials, presenting each word/referent 3-10 times across a total span of <10 minutes.
After a training procedure, knowledge is typically tested by presenting each word, and asking learners to choose the best referent, either from the entire set or from a small subset of referents (e.g., 4-alternative forced choice; 4AFC).
On the whole, these paradigms have found that people of all ages are able to learn the meanings of novel words from the series of individually-ambiguous scenes at a level significantly above chance, but far from perfect.
Numerous training manipulations have been found to influence performance: e.g., how often pairs are presented [although this is modulated by the diversity of the contexts in which the pairs appear; @Suanda2012; @Kachergis2016], the inter-trial lag between a pair's appearance [@Vlach2012eq; @Vlach2013], and the number of pairs presented per trial [@Yu2008] all predict learning performance to some extent.
In sum, the evidence is strong that people can learn word cross-situationally: we are somehow able to integrate evidence (co-occurrence of words and objects) across trials and use that evidence to learn and remember new word meanings.
But *how* do people carry out such learning? 

Along with the great experimental interest in the phenomenon of cross-situational word learning, there has also been great interest in creating and testing formal computational models of *how* learners selectively attend to, store, and recall knowledge of word meanings.
These models often represent distinct intuitions of how learning operates, variously representing the learning problem as one of logical inference [e.g., @Siskind1996], as one of proposing and testing hypotheses [e.g., @Trueswell2013], as one of accumulating graded associations [e.g., @Fazly2010; @Kachergis2012gi; @tilles2012minimal]. <!-- or as a blend of the former -->
The extant cross-situational word learning models are too numerous to exhaustively cite here, and while each has been tested against at least a few datasets, it is unclear which of these models--or even a broad class of them, or one component mechanism--are best fit to explain the wide variety of findings.
There is even some suggestion that under certain assumptions models representing very different intuitions (e.g., storing a single hypothesized referent for each word vs. accumulating a word x referent association matrix representing all experienced co-occurrences) can mimic each other in some experimental conditions, making the models unidentifiable [@Yu2012hyp].
However, we believe that comparing a large selection of models fitted jointly to a wide variety of experimental conditions may reveal which model(s, classes, mechanism) are most promising for explaining human word learning behavior.

Thus, the goal of this study is to test the ability of several cross-situational word learning models to account for a wide range of experimental data using cross-validated parameter fits.
Our goal is to scale up both the number of models evaluated and the number of experiment designs (and participants) modeled, recognizing that the plurality of models and the theories and intuitions they represent can only be winnowed down via rigorous comparison.
We will find best-fitting parameters for associative models, hypothesis-testing models, and a few hybrid models, and will quantify generalization on held-out data, as well as qualitatively in other experiments.
Finally, we will interpret the results, and what they entail for theories of cross-situational word learning.

# Method

## Models

```{r, model-list}
# need to be run >100 times per given parameter values
stochastic_models = list.files(here("models/stochastic/"))
# assoc models need to be run only once per given parameter values
assoc_models = list.files(here("models/"))
assoc_models = assoc_models[assoc_models!="stochastic"]
```

We fit the following associative models: Fazly [@Fazly2010], Kachergis [@Kachergis2012gi,Kachergis2012vc], ... and a baseline co-occurrence counting model.
We also fit two hypothesis-testing models: guess-and-test [@Medina2011,@Blythe2010] and propose-but-verify [@Trueswell2013].
Finally, we fit two hybrid models, that store multiple, graded hypotheses--but only a subset of all possible associations: pursuit [@Stevens2017pursuit], and a stochastic version of the Kachergis model.
These models and their free parameters are described below.

### Propose-but-verify
In the propose-but-verify hypothesis testing model @Trueswell2013, one of the presented referents is selected at random for any word heard that has no remembered referent. 
The next time that word occurs, the previously-proposed referent is remembered with probability $\alpha$, a free parameter. 
If the remembered referent is verified to be present, the future probability of recalling the word is increased by $\epsilon$ (another free parameter). 
If the remembered referent is not present, the old hypothesis is assumed to be forgotten and a new proposal is selected from the available referents.
This model implements trial-level mutual exclusivity by selecting new proposals only from among the referents that are not yet part of a hypothesis.

### Guess-and-test
The guess-and-test hypothesis testing model is based on the description given by @Medina2011 of a one-shot (i.e. "fast mapping") learning, which posits that "i) learners hypothesize a single meaning based on their first encounter with a word, ii) learners neither weight nor even store back-up alternative meanings, and iii) on later encounters, learners attempt to retrieve this hypothesis from memory and test it against a new context, updating it only if it is disconfirmed."
In summary, guess-and-test learners do *not* reach a final hypothesis by comparing multiple episodic memories of prior contexts or multiple semantic hypotheses.
We give this model two free parameters: a probability of successful encoding ($s$, hypothesis formation), and a probability $f$ of forgetting a hypothesis at retrieval. 
This model is quite similar to the guess-and-test model formally analyzed by @Blythe2010 to determine the theoretical long-term efficacy of cross-situational word learning.

## Data

```{r load-data}
load(here("data/combined_data.RData"))

tab <- tibble("Name"=NA, "Trials"=NA, "Words/Tr"=NA, "Objs/Tr"=NA, "Items"=NA, "Accuracy"=NA, "SD"=NA, "N"=NA) 

totSs = 0
totItems = 0
for(c in 1:length(combined_data)) { 
  totSs = totSs + combined_data[[c]]$Nsubj 
  totItems = totItems + length(combined_data[[c]]$HumanItemAcc)
  tab <- bind_rows(tab, c("Name" = combined_data[[c]]$Condition, 
                          "Trials" = nrow(combined_data[[c]]$train$words), # trials
                          "Words/Tr" = ncol(combined_data[[c]]$train$words), # words
                          "Objs/Tr" = ncol(combined_data[[c]]$train$objs), # objs
                          "Items" =length(combined_data[[c]]$HumanItemAcc),
                          "Accuracy" = mean(combined_data[[c]]$HumanItemAcc),
                          "SD" = sd(combined_data[[c]]$HumanItemAcc),
                          "N" = combined_data[[c]]$Nsubj
                          ))
}

tab = na.omit(tab)
```

The modeled data are average accuracies from `r totItems` word-object pairs in `r length(combined_data)` experimental conditions, in which a total of `r totSs` subjects participated.
Table 1 shows a variety of characteristics of each condition's training and test procedures, including the number of trials, the number of words and referents presented per training trial, the number of total to-be-learned pairs, the number of participants per condition, and the average accuracy.
Most of these data have been previously published: data from 13 of the conditions were reported in @Kachergis2013nat, data from four conditions are from @Kachergis2012gi, and from another four conditions are from @Kachergis2016.



Each condition consists of an ordered list of training trials consisting of 1-4 words and 2-4 objects per trial.

## Procedure

We will first optimize each model's parameters with 5-fold cross-validation, leaving out data from 20% of the conditions to test generalization from optimizing to the remaining 80%.
We will then optimize each model's parameterss to the entire dataset, and use these parameters to test generalization to a selection of other experiments from the literature for which we do not have item-level accuracy.

## Results

```{r load-cv-fits}

load(here("fits/cv_group_fits.Rdata"))
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg_fit_tab <- cvg %>% group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)
```


```{r cv-fits-table, results="asis"}
# how to do 2-col table? , fig.env = "figure*" 
tab2 <- xtable::xtable(cvg_fit_tab, digits=3, 
                       caption = "Cross-validated group fits.")

print(tab2, type="latex", comment = F, include.rownames = F, table.placement = "p")
```

You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`.

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}
#img <- png::readPNG("figs/walrus.png")
#grid::grid.raster(img)
```

## One-column images

Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`.

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."}
#img <- png::readPNG("figs/lab_logo_stanford.png")
#grid::grid.raster(img)
```




# Discussion



# Acknowledgements

Thanks to Chen Yu for allowing us to include data from several experimental conditions.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

```{r, child = "appendix.Rmd"}
```
