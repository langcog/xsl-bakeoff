---
#title: "Appendix"
output: pdf_document
---

```{r, echo=F, warning=F, message=F}
require(here)
require(gplots)
require(tidyverse)
require(RColorBrewer)
knitr::opts_chunk$set(echo=F, message=F, warning=F)
```

# Appendix A: Summary of Experimental Conditions

Table 1 describes the 44 experimental conditions included in the model comparison, including the number of trials, the number of words presented per trial (Words/Trial), the number of referents presented per trial (Objects/Trial), the number of to-be-learned word-referent pairs (Items), people's overall mean accuracy ($p(o|w)$ across all intended $w-o$ mappings) in each condition (Accuracy), the standard deviation of performance (SD), and the number of participants per condition (N). 

```{r conditions-table, results="asis", echo=F}
load(here("data/combined_data.RData"))

tab <- tibble("Condition Name"=NA, "Trials"=NA, "Words/Trial"=NA, "Objects/Trial"=NA, "Items"=NA, "Accuracy"=NA, "SD"=NA, "N"=NA) 

totSs = 0
totItems = 0
for(c in 1:length(combined_data)) { 
  totSs = totSs + combined_data[[c]]$Nsubj 
  totItems = totItems + length(combined_data[[c]]$HumanItemAcc)
  tab <- bind_rows(tab, c("Condition Name" = combined_data[[c]]$Condition, 
                          "Trials" = nrow(combined_data[[c]]$train$words), # trials
                          "Words/Trial" = ncol(combined_data[[c]]$train$words), # words
                          "Objects/Trial" = ncol(combined_data[[c]]$train$objs), # objs
                          "Items" =length(combined_data[[c]]$HumanItemAcc),
                          "Accuracy" = mean(combined_data[[c]]$HumanItemAcc),
                          "SD" = sd(combined_data[[c]]$HumanItemAcc),
                          "N" = combined_data[[c]]$Nsubj
                          ))
}

tab = na.omit(tab)
tab <- tab %>% mutate(Accuracy = as.numeric(tab$Accuracy),
               SD = as.numeric(SD))
tab1 <- xtable::xtable(tab, digits=2, 
                       caption = "Summary of modeled datasets.")

print(tab1, type="latex", comment = F, include.rownames = F, table.placement = "p")
```

# Appendix B: Clustering experiments and models by misfit

The heatmap below shows cross-validated model fit (SSE; sum of squared error) for each experimental condition.
Note that each model has difficulty fitting at least one ore more experimental conditions--and these difficult conditions vary somewhat by model.
For example, the strength-biased model has particular difficulty with the `filt` conditions[^1] (Kachergis et al., 2012), which present a group of word-referent pairs early in training which then systematically co-occur with particular novel late-stage word-referent pairs, testing how strictly learners will maintain a mutual exclusivity (ME) constraint. 
The Trueswell2012 model also shows greater misfit in most of these conditions (except for the `filtXE_3L` conditions, which have only 3 repetitions of the late-stage pairings, and thus do not overwhelm learners' ME bias).

On the other hand, many experimental conditions are nearly equally well fit by all models, especially those that have a fixed number of repetitions per word-referent pair (e.g., 3x3 and 4x4, although 2x2 presents difficulties for some models).

[^1]: Except for the `filt0E_` conditions, which consist only of the late-stage pairs, with no early stage.

```{r, fig.width=8, fig.height=9}
load(here("fits/cv_group_fits.Rdata"))
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg <- cvg %>% mutate(Model = case_when(Model == 'kachergis' ~ 'Fam./Unc.',
                             Model == 'fazly' ~ 'Prob. Assoc.',
                             Model == 'novelty' ~ 'Novelty',
                             Model == 'Bayesian_decay' ~ 'Bayesian Decay', 
                             Model == 'strength' ~ 'Strength',
                             Model == 'uncertainty' ~ 'Uncertainty',
                             Model == 'rescorla-wagner' ~ 'Rescorla-Wagner',
                             Model == 'trueswell2012' ~ 'Propose-but-Verify',
                             Model == 'guess-and-test' ~ 'Guess-and-Test',
                             Model == 'kachergis_sampling' ~ 'Fam./Unc. sampling',
                             Model == 'pursuit' ~ 'Pursuit',
                             Model == 'baseline' ~ 'Baseline',
                          TRUE ~ Model)) 

exp_by_mod <- cvg %>% 
  mutate(SSE = (ModelPerf-HumanPerf)^2) %>% # should we weight be sample size?
  group_by(Condition, Model) %>%
  summarize(meanSSE = mean(SSE)) %>%
  pivot_wider(id_cols = Condition, names_from = Model, values_from = meanSSE)

xm_mat <- as.matrix(exp_by_mod[,2:12])
row.names(xm_mat) = exp_by_mod$Condition


colfunc <- colorRampPalette(c("black", "red")) 
heatmap.2(xm_mat, col=colfunc(20), cexCol=.85, srtCol=45, margins = c(6,6.5)) # , scale="row"
```



<!-- Other ideas: -->
<!-- - Find experiment with maximal discrimination between the models? (highest SD of fit?) -->

# Correlation of model vs. human item-level performance per condition

Each cell displays the correlation coefficient of model vs. human item-level performance in a given experimental condition.

```{r, fig.width=8, fig.height=9}
exp_by_mod <- cvg %>%
  group_by(Condition, Model) %>%
  summarize(r = cor(ModelPerf, HumanPerf)) %>%
  pivot_wider(id_cols = Condition, names_from = Model, values_from = r)

xm_mat <- as.matrix(exp_by_mod[,2:12])
row.names(xm_mat) = exp_by_mod$Condition
require(pals)
heatmap.2(xm_mat, col=brewer.spectral(30), # brewer.pal(11,"RdBu")
          scale="none", trace="none", 
          cexCol=.85, srtCol=45, margins = c(6,6.5)) # scale="row", "col" <- interesting
```


## Each Model's Best- and Worst-Fitting Experiment

Table 2 summarizes for each model, what experimental condition is best fit by that model (in terms of correlation, `best` and `best_cond`), and what condition has the worst fit for that model (`worst` and `worst_cond`).
For six models, `freq369-3x3loC` was the easiest to fit.
For four models, `filt6E_3Lfazly` was the most difficult to fit.
<!-- Which experiments are models best fitting? -->

```{r, results="asis"}
exp_by_mod <- cvg %>%
  group_by(Condition, Model) %>%
  summarize(r = cor(ModelPerf, HumanPerf)) 

models = unique(exp_by_mod$Model)
conditions = unique(exp_by_mod$Condition)

  
# NAs for fazly, rescorla-wagner, and Bayesian decay for a few filt conditions??
# exp_by_mod[which(is.na(exp_by_mod$r)),]
#subset(cvg, Condition=="filt0E_3L" & Model=="fazly") # no variation in model performance: ModelPerf=.402
#subset(cvg, Condition=="filt0E_3L" & Model=="rescorla-wagner") # ModelPerf=.5
#subset(cvg, Condition=="filt0E_3L" & Model=="Bayesian_decay") # ModelPerf=.237

mod_best_worst <- exp_by_mod %>% group_by(Model) %>%
  summarize(best = max(r, na.rm=T), 
            worst = min(r, na.rm=T),
            best_cond = Condition[which(r==best)],
            worst_cond = Condition[which(r==worst)]) %>%
  arrange(desc(best))

tab2 <- xtable::xtable(mod_best_worst, digits=2, 
                       caption = "Each model's best- and worst-fitting experiment.")

print(tab2, type="latex", comment = F, include.rownames = F, table.placement = "p")
```



```{r, eval=F}
sort(table(mod_best_worst$best_cond)) 
# freq369-3x3loCD best for 6 models; freq369_36mx best for 2 models

sort(table(mod_best_worst$worst_cond))
# filt6E_3L worst condition for 4 models; 2x3 6/9 4AFC worst for 2
# and 3 other asymmetric conditions worst for 3 other models: 1x3, 2x3, 3x4
```


<!-- ## Best and worst fitting models per experiment -->

```{r, results="asis", include=F}
cond_best_worst <- exp_by_mod %>% group_by(Condition) %>%
  summarize(best = max(r, na.rm=T), 
            worst = min(r, na.rm=T), 
            best_mod = Model[which(r==best)],
            worst_mod = Model[which(r==worst)]) %>%
  arrange(desc(best))

#sort(table(cond_best_worst$best_mod)) # fazly best for 6 conds, trueswell2012 for 5, kachergis_sampling for 5
# freq369-3x3loCD best for 6 models; freq369_36mx best for 2 models
#sort(table(cond_best_worst$worst_mod)) # rescorla-wagner worst for 8 conditions, guess-and-test for 7, pursuit for 6

best <- cond_best_worst %>%
  group_by(best_mod) %>% summarize(best_fits = n()) %>%
  rename(Model = best_mod)

worst <- cond_best_worst %>%
  group_by(worst_mod) %>% summarize(worst_fits = n()) %>%
  rename(Model = worst_mod)

best_worst_tab <- best %>% left_join(worst)

tab3 <- xtable::xtable(best_worst_tab, digits=2, 
                       caption = "Number of experiments that each model fits best, and worst.")

print(tab3, type="latex", comment = F, include.rownames = F, table.placement = "p")
```

