---
title: "XSL Model Bake-off"
author: "George Kachergis"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning=F, message=F)
require(tidyverse)
require(ggplot2)
library(knitr)
library(kableExtra)
```

## Introduction

The goal is to test a variety of cross-situational word learning models from the recent literature on a range of findings. We will examine both associative models and hypothesis-testing models, and will find optimal parameters both globally (for all experiments), as well as per condition, and perhaps even per participant.

Models:
* baseline count model?
* propose-but-verify
* guess-and-test
* pursuit model
* Kachergis model (associative + sampling version)
  + strength bias model
  + uncertainty bias model
  + novelty bias model
* Fazly et al.
* "Bayesian" decay
* Tilles & Fontanari 2013 (ToDo: verify)
* Yurovsky & Frank 2015 (ToDo!!)


### The Data

I have participant-level data for: 1) 556 subjects from 18 conditions (Kachergis & Yu, 2013), 2) frequency and contextual diversity experiments (including a multi-block version), and 3) 

We will first optimize each model's parameters to the entire dataset (all 1532 participants), as well as to each condition.
We will then conduct generalization experiments to see how well each model (with earlier optimized parameters) predicts the result of other experiments in the literature, for which we do not have participant-level data.


```{r, load-data}
# individual-level subject data for 18 conditions (from Kachergis & Yu 2013)
load("data/asym_master.RData") 
asym <- raw %>% group_by(Condition, CondNum, Subject) %>%
  summarise(perf = mean(Correct)) %>%
  group_by(Condition, CondNum) %>% summarise(Nsubj=n()) 
# CondNums with missing names: 
# 217 - 2 words x 4 objects, 54 trials
# 218 - 3 words x 4 objects, 36 trials
# 220 - 3 words x 4 objects, 54 trials
# sum(asym$Nsubj) # 556 subjects

load("data/asymmetric_conditions.RData") 
# conds = 

load("data/master_orders.RData") # trial orders for ME (filt) and FreqCD papers
# orders

# multi-block experiment: test generalization?
load("data/priming_all_trajectory.RData") 
# 86 subjects on 4 blocks of equal frequency 4pairs/trial, or 3,6,9x freq 3 or 4 pairs per trial
asym <- raw %>% group_by(Condition, CondNum, Subject) %>%
  summarise(perf = mean(Correct)) %>%
  group_by(Condition, CondNum) %>% summarise(Nsubj=n())
```

For each condition, need an ordered list of training trials consisting of 1-4 words and 2-4 objects per trial.
We also require a test function--often presenting each word a single time along *m*AFC of objects, where *m* is the number of objects seen during the training, but sometimes a subset of the objects are tested.

### Model Descriptions

Each model accepts a condition's list of training trials, as well as 2 or 3 parameters.

```{r, model-list}
# need to be run >100 times per given parameter values
stochastic_models = list.files("models/stochastic/")
# assoc models need to be run only once per given parameter values
assoc_models = list.files("models/") 
assoc_models = assoc_models[assoc_models!="stochastic"]

```

```{r, load-fits}
#source("fitting_functions.R")

# each model jointly fit to all conditions
load("fits/group_fits.Rdata")
#for(m in names(group_fits)) { 
#  print(paste(m, round(group_fits[[m]]$optim$bestval, 2)))
#}
# each model separately fit to each condition (should try cross-validation approach?)

load("fits/cond_fits.Rdata")

#load("fits/individual_fits.Rdata")
# not all conditions have subject-level data (but I think I can track it down)
# moreover, some subjects participated in multiple conditions, but I'm not sure 
# I can trace that for all of the conditions (certainly for the exps I ran -- ~25 conditions)

load("fits/cv_group_fits.Rdata")
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg_fit_tab <- cvg %>% group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)


```

## Model Fitting Procedure

We will fit each model with two different cross-validation methods.
First, data from all 46 conditions

# Results


## Conditions-left-out Cross-validated Group Fits


```{r}
kable(cvg_fit_tab, digits=3, caption="Group model fits.")
```

## Group-level Model Fits

```{r, summarize-group-fits}
group_fit_tab <- gfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)

kable(group_fit_tab, digits=3, caption="Group model fits.") 
```


Plotted by condition means.

```{r, plot-group-fits-avg, fig.width=8}
# look at condition average fits
ccondm <- cfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#ccondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)


gcondm <- gfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#gcondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)

gcondm %>%
  ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition, size=Nsubj)) + 
  geom_point(alpha=.7) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")
  #theme(legend.position="bottom")
```

Plotted by individual items.

```{r, plot-group-fits, fig.width=8, caption="Model vs. human performance by model and condition for group model fits."}
gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition)) + 
  geom_point(alpha=.8) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")

#gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Model, color=Model)) + 
#  geom_point() + facet_wrap(vars(Condition)) + theme_bw() + geom_smooth(method = "lm")
```

## Per-Condition Model Fits
(not done yet for sampling models, and we may not even want to do this except for a version with cross-validation)

```{r, summarize-condition-fits}
cond_fit_tab <- cfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)

kable(cond_fit_tab, digits=3, caption="By-condition model fits.") 
```

```{r, plot-cond-fits, fig.width=8, caption="Model vs. human performance by model and condition for per-condition model fits."}
cfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition)) + 
  geom_point(alpha=.8) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")
```


## Generalization Experiments

Using the parameters optimized to each model for all of the conditions, we simulate model performance for a selection of published experiments and compare to participants' group-level performance for these conditions. 
The first three studies are of adult participants, and include two experiments that have been interpreted as supporting hypothesis-testing theories of word learning.


### Koehne, Trueswell & Gleitman (2013)
For example, Koehne, Trueswell & Gleitman (2013), in which each of 16 novel nouns was assigned two meanings with different co-occurrence frequencies: 
One referent was present whenever the noun was present (six times, 100% referent), the other referent was present in only half of the cases the noun was (three times, 50% referent). All other objects co-occurred only once with a noun (17%). 
Training trial order was manipulated: including and excluding the 50% referent were presented within four levels (within participants): 
Firstly, the 50%-present (P) and 50%-absent (A) trials could be either blocked (AAAPPP and PPPAAA) or not blocked (APAPAP and PAPAPA); secondly, the first encounter of a noun could be either an A trial (AAAPPP and APAPAP) or a P trial (PPPAAA and PAPAPA).

```{r}
load("other_exps/KoehneTrueswellGleitman2013orders.RData")

# ToDo: modify to work for stochastic models, too
run_Koehne_exp <- function(model_name, par, ord) {
  source(paste0("models/",model_name,".R"))
  mp = model(par, ord)$matrix
  diag(mp[,1:8])/rowSums(mp[,1:8]) # 100% referents: ~60% perf
  diag(mp) = 0
  perf = c()
  for(w in 1:nrow(mp)) {
    perf[w] = mp[w,w+1]/sum(mp[w,1:9])
  }
  return(perf) 
}


evalSSE_Koehne <- function(model_name, par, orders, verbose=F) {
  df = tibble(Cond = names(Koehne), Human=rep(0.0, 4), Model=rep(0.0, 4), Model_sd=rep(0.0, 4))
  for(i in 1:length(Koehne)) {
    cond = names(Koehne)[i]
    # words needs to be an Nx1 matrix
    Koehne[[cond]]$train$words = matrix(Koehne[[cond]]$train$words, ncol=1)
    mperf = run_Koehne_exp(model_name, par, Koehne[[cond]]$train)
    df[i,]$Model = mean(mperf)
    df[i,]$Model_sd = sd(mperf)
    df[i,]$Human = Koehne[[cond]]$HumanAcc
  }
  if(verbose) print(df)
  return(sum((df$Human-df$Model)^2)/4)
}

gen_exps = tibble(exp=character(), model=character(), SSE=numeric())
for(model_name in names(group_fits)[1:6]) {
  pars = group_fits[[model_name]]$optim$bestmem
  sse = evalSSE_Koehne(model_name, pars, Koehne, verbose=F) # 
  gen_exps = bind_rows(gen_exps, tibble(exp="Koehne et al.", model=model_name, SSE=sse))
}

# run_Koehne_exp("kachergis", c(0.1077372, 14.99996, 1), Koehne[["pppaaa"]]$train)


# the model predicts the same ordering as humans out of the box -- PPPAAA is best, AAAPPP is worst
# contradicts Koehne et al.'s interpretation: "This finding is inconsistent with a standard cross-situational
# account because all conditions should have been above chance independent of presentation order.

```


### Medina et al. 2011 
Medina et al. 2011 tasked adult participants with learning 12 nonce words presented across 60 training trials presenting only one word at a time.
The number of referents per training trial varied: High Informative (HI) trials showed two referents, while Low Informative (LI) trials showed five referents.
Unlike most cross-situational word learning experiments, each word depicted not a single referent, but instead corresponded to a category of five referents, each appearing  (e.g., "bosa" might appear with five pictures of bears on as many separate trials).
Different between-subjects conditions presented 12 HI trials (one per word) at the beginning, middle, or end of training, or not at all (HI-absent).

```{r, medina-exp}
load("other_exps/Medina2013orders.RData")
# HIfirst: 66% accuracy on vignette 1; 41% accuracy by the fifth vignette; offer this guess as “the meaning of the word” at experiment’s end (37% accuracy on the Final Guess)
# they don't list the means per cond/vignette, except in Fig2. 
# I tried to read off the performance from the Figure...
Medina[["HIfirst"]]$HumAcc = c(.66, .33, .39, .46, .41) # final: .37
Medina[["HImid"]]$HumAcc = c()
Medina[["HIlast"]]$HumAcc = c()
Medina[["HIabsent"]]$HumAcc = c()
  
# graph performance per block (1-5)
get_block_perf <- function(modperf) {
  perf_inds = seq(12,60,12)
  perf = c()
  for(i in perf_inds) {
    perf = c(perf, mean(diag(modperf$traj[[i]]) / rowSums(modperf$traj[[i]])))
  }
  return(perf)
}

run_Medina_exp <- function(model_name, pars, plot=T) {
  source(paste0("models/",model_name,".R"))
  HI1 = model(pars, Medina[["HIfirst"]]$train)
  HIm = model(pars, Medina[["HImid"]]$train)
  HIl = model(pars, Medina[["HIlast"]]$train)
  HIabs = model(pars, Medina[["HIabsent"]]$train)

  mdat = data.frame(rbind(cbind(Condition="HI first", Vignette=1:5, Performance=get_block_perf(HI1)),
        cbind(Condition="HI middle", Vignette=1:5, Performance=get_block_perf(HIm)),
        cbind(Condition="HI last", Vignette=1:5, Performance=get_block_perf(HIl)),
        cbind(Condition="HI absent", Vignette=1:5, Performance=get_block_perf(HIabs)) ))
  mdat$Performance = as.numeric(as.character(mdat$Performance))
  mdat$Model = model_name
  
  if(plot) {
    p <- ggplot(mdat, aes(x=Vignette, y=Performance, group=Condition, shape=Condition, color=Condition)) + 
      geom_point() + geom_line() + theme_bw() + ylim(c(0,1.0))
    print(p)
  }
  return(mdat)
}

run_Medina_exp("kachergis", c(.126, 14.25, 1.0))
run_Medina_exp("fazly", c(0.4695008, 3.690278)) # Error in rowSums(modperf$traj[[i]]) : 'x' must be an array of at least two dimensions

# ToDo: write evalSSE_Medina()
```


### Yu, Zhong, and Fricker (2012)
Yu, Zhong, and Fricker (2012) pre-trained adult participants with three word-object associations and found higher performance on the other 15 words after this pre-training, demonstrating that knowing even a few word meanings can improve learning for other co-occurring words. 

```{r, pretraining-exp}
# orig4x4, but 3 pairs (which ones??) are pretrained: accuracy on those is .9187, 
# and accuracy on the other 15 is .5812 
# How variable is this effect based on selecting different 3 items for retraining?)
yuzhong_emp = c(.9187, .5812)
yuzhong_ord = orders[["orig_4x4"]]$train

# pre-train a random subset of n_pretrained word-object pairs
run_pretrain_experiment <- function(parms, n_pretrained=3, start_val=1) {
  voc_sz = 18
  start_matrix = matrix(0, voc_sz, voc_sz)
  pretrain_inds = sample(1:18, n_pretrained)
  other_inds = setdiff(1:18, pretrain_inds)
  #start_matrix[pretrain_inds, pretrain_inds] = .01 # irrelevant to seed off-diagonal or not
  diag(start_matrix)[pretrain_inds] = start_val
  mm = model(parms, yuzhong_ord, start_matrix = start_matrix) 

  return(c(mean(mm$perf[pretrain_inds]), mean(mm$perf[other_inds])))
}

# run Nsim pretraining experiments (with different random pretrained items)
run_pretrain_experiment_batch <- function(model_name, parms, n_pretrained=3, start_val=1, Nsim=100) {
  source(paste0("models/",model_name,".R"))
  dat = data.frame()
  for(i in 1:Nsim) {
    dat = rbind(dat, round(run_pretrain_experiment(parms, n_pretrained=n_pretrained, start_val=start_val), 3))
  }
  names(dat) = c("pretrain", "other")
  return(dat)
}

source("models/kachergis.R")

pt3 = run_pretrain_experiment_batch(c(.1, 2, .92), n_pretrained=3, start_val=1.6, Nsim=200)
colMeans(pt3) # pretrained = .89, other = .55

sum((colMeans(pt3) - yuzhong_emp)^2) # .002

pt6 = run_pretrain_experiment_batch(c(.1, 2, .92), n_pretrained=6, start_val=1.6, Nsim=200)
colMeans(pt6) # pretrained = .89, other = .66

pt9 = run_pretrain_experiment_batch(c(.1, 2, .92), n_pretrained=9, start_val=1.6, Nsim=200)
colMeans(pt9) # pretrained = .89, other = .76

```


### Suanda et al. (2014)
Suanda et al. (2014) varied contextual diversity in a cross-situational word learning study of 5- to 7-year-olds, presenting children with 8 to-be-learned words shown two per trial across a total of 16 trials.
...2AFC?
```{r, suanda-exp}
# Suanda et al. 2014 - contextual diversity effects in children
load("other_exps/Suanda2014orders.RData") # Suanda orders and HumanAcc

# Suanda[["Low CD"]]: each pair appears 3 times with another pair and once with a 2nd pair
# Medium CD: each pair appears once with two other pairs and twice with another pair
# High CD: each pair appears with 4 other pairs

evalSSE_orders <- function(model_name, par, orders, verbose=F) {
  df = tibble(Cond = character(), Human=numeric(), Model=numeric(), Model_sd=numeric())
  for(i in 1:length(orders)) {
    cond = names(orders)[i]
    mperf = model(par, orders[[cond]]$train)$perf
    df = bind_rows(df, tibble(cond=cond, Model=mean(mperf), Model_sd=sd(mperf), 
                              Human=orders[[cond]]$HumanAcc))
  }
  if(verbose) print(df)
  return(sum((df$Human-df$Model)^2)/length(orders))
}

for(model_name in names(group_fits)[1:6]) {
  pars = group_fits[[model_name]]$optim$bestmem
  sse = evalSSE_orders(model_name, pars, Suanda, verbose=F) # 
  gen_exps = bind_rows(gen_exps, tibble(exp="Suanda et al.", model=model_name, SSE=sse))
}
# fazly: Error in if (min(p) < 0 || sum(p) <= 0) return(NA) : missing value where TRUE/FALSE needed
```


### Vlach & DeBrock (2017, 2019)
12 word-object pairs, 2 pairs/trial, 36 trials
6 pairs massed, 6 interleaved
2AFC testing

```{r, vlach-exp}
vlach_ord <- read.csv("other_exps/VlachDeBrock2017_2019.txt", header=F, sep='\t')
# 2017 paper had immediate testing: 2–3 year-olds (61% correct responses), 4–5 year-olds (67% correct)
#  - no diff between interleaved/massed with the immediate test
# 2019 paper introduced a 5-minute delay between learning and test phases (children aged 25-58 mos), M=.57
#  - sig. higher performance on the interleaved word-object mappings

massed = c(1, 8, 9, 10, 11, 12)
spaced = setdiff(1:12, massed)
# 2AFC test, overall M=6.81 words learned (better with age: only 47-58 mos / 50-70 mos groups above chance
# Exp1: massed M=3.17, interleaved M=3.64 
# Exp2: massed M=3.00, interleaved M=3.48

mp = model(c(.003, 2, .97), vlach_ord)
mperf = mAFC_test(mp$matrix, 2)
massed_perf = sum(mperf[massed]) 
spaced_perf = sum(mperf[spaced]) 
```


### Smith & Yu 2008 / Yu & Smith (2011)
6 word-object pairs, 2 pairs/trial, 30 trials

```{r}
sy_ord <- read.csv("other_exps/smithYu2008-2x2-6w.txt", header=F, sep='\t')


mAFC_test <- function(mat, m) {
  perf = rep(0, nrow(mat))
  for(i in 1:nrow(mat)) {
    # sample m-1 incorrect referents
    alts = sample(setdiff(1:ncol(mat), i), m-1)
    perf[i] = mat[i,i] / sum(mat[i,c(alts, i)])
  }
  return(perf)
}

mp = model(c(.0015, 2, .97), sy)
sum(mAFC_test(mp$matrix, 2)) # 3.57 words learned

mp = model(c(.003, 1, .97), sy)
sum(mAFC_test(mp$matrix, 2)) # 4.00 words learned
```

