---
title: "XSL Model Bake-off"
author: "George Kachergis"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning=F, message=F)
require(tidyverse)
require(ggplot2)
library(knitr)
library(kableExtra)
```

## Introduction

The goal is to test a variety of cross-situational word learning models from the recent literature on a range of findings. We will examine both associative models and hypothesis-testing models, and will find optimal parameters both globally (for all experiments), as well as per condition, and perhaps even per participant.

Models:
* baseline count model?
* propose-but-verify
* guess-and-test
* pursuit model
* Kachergis model (associative + sampling version)
  + strength bias model
  + uncertainty bias model
  + novelty bias model
* Fazly et al.
* "Bayesian" decay
* Tilles & Fontanari 2013 (ToDo: verify)
* Yurovsky & Frank 2015 (ToDo!!)


### The Data

I have participant-level data for: 1) 556 subjects from 18 conditions (Kachergis & Yu, 2013), 2) frequency and contextual diversity experiments (including a multi-block version), and 3) 

We will first optimize each model's parameters to the entire dataset (all 1532 participants), as well as to each condition.
We will then conduct generalization experiments to see how well each model (with earlier optimized parameters) predicts the result of other experiments in the literature, for which we do not have participant-level data.


```{r, load-data}
# individual-level subject data for 18 conditions (from Kachergis & Yu 2013)
load("data/asym_master.RData") 
asym <- raw %>% group_by(Condition, CondNum, Subject) %>%
  summarise(perf = mean(Correct)) %>%
  group_by(Condition, CondNum) %>% summarise(Nsubj=n()) 
# CondNums with missing names: 
# 217 - 2 words x 4 objects, 54 trials
# 218 - 3 words x 4 objects, 36 trials
# 220 - 3 words x 4 objects, 54 trials
# sum(asym$Nsubj) # 556 subjects

load("data/asymmetric_conditions.RData") 
# conds = 

load("data/master_orders.RData") # trial orders for ME (filt) and FreqCD papers
# orders

# multi-block experiment: test generalization?
load("data/priming_all_trajectory.RData") 
# 86 subjects on 4 blocks of equal frequency 4pairs/trial, or 3,6,9x freq 3 or 4 pairs per trial
asym <- raw %>% group_by(Condition, CondNum, Subject) %>%
  summarise(perf = mean(Correct)) %>%
  group_by(Condition, CondNum) %>% summarise(Nsubj=n())
```

For each condition, need an ordered list of training trials consisting of 1-4 words and 2-4 objects per trial.
We also require a test function--often presenting each word a single time along *m*AFC of objects, where *m* is the number of objects seen during the training, but sometimes a subset of the objects are tested.

### Model Descriptions

Each model accepts a condition's list of training trials, as well as 2 or 3 parameters.

```{r, model-list}
# need to be run >100 times per given parameter values
stochastic_models = list.files("models/stochastic/")
# assoc models need to be run only once per given parameter values
assoc_models = list.files("models/") 
assoc_models = assoc_models[assoc_models!="stochastic"]

```

```{r, load-fits}
#source("fitting_functions.R")

# each model jointly fit to all conditions
load("fits/group_fits.Rdata")
#for(m in names(group_fits)) { 
#  print(paste(m, round(group_fits[[m]]$optim$bestval, 2)))
#}
# each model separately fit to each condition (should try cross-validation approach?)

load("fits/cond_fits.Rdata")

#load("fits/individual_fits.Rdata")
# not all conditions have subject-level data (but I think I can track it down)
# moreover, some subjects participated in multiple conditions, but I'm not sure 
# I can trace that for all of the conditions (certainly for the exps I ran -- ~25 conditions)

load("fits/cv_group_fits.Rdata")
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg_fit_tab <- cvg %>% group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)


```

## Model Fitting Procedure

We will fit each model with two different cross-validation methods.
First, data from all 46 conditions

# Results


## Conditions-left-out Cross-validated Group Fits


```{r}
kable(cvg_fit_tab, digits=3, caption="Group model fits.")
```

## Group-level Model Fits

```{r, summarize-group-fits}
group_fit_tab <- gfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)

kable(group_fit_tab, digits=3, caption="Group model fits.") 
```


Plotted by condition means.

```{r, plot-group-fits-avg, fig.width=8}
# look at condition average fits
ccondm <- cfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#ccondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)


gcondm <- gfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#gcondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)

gcondm %>%
  ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition, size=Nsubj)) + 
  geom_point(alpha=.7) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")
  #theme(legend.position="bottom")
```

Plotted by individual items.

```{r, plot-group-fits, fig.width=8, caption="Model vs. human performance by model and condition for group model fits."}
gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition)) + 
  geom_point(alpha=.8) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")

#gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Model, color=Model)) + 
#  geom_point() + facet_wrap(vars(Condition)) + theme_bw() + geom_smooth(method = "lm")
```

## Per-Condition Model Fits


```{r, summarize-condition-fits}
cond_fit_tab <- cfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)

kable(cond_fit_tab, digits=3, caption="By-condition model fits.") 
```

```{r, plot-cond-fits, fig.width=8, caption="Model vs. human performance by model and condition for per-condition model fits."}
cfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition)) + 
  geom_point(alpha=.8) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")
```


## Generalization Experiments

Using the model parameters obtained above, we simulate model performance for a selection of published experiments and compare to participants' group-level performance for these conditions. 
The first three studies are of adult participants, and include two experiments that have been interpreted as supporting hypothesis-testing theories of word learning.

### Koehne, Trueswell & Gleitman (2013)
For example, Koehne, Trueswell & Gleitman (2013), in which each of 16 novel nouns was assigned two meanings with different co-occurrence frequencies: 
One referent was present whenever the noun was present (six times, 100% referent), the other referent was present in only half of the cases the noun was (three times, 50% referent). All other objects co-occurred only once with a noun (17%). 
Training trial order was manipulated: including and excluding the 50% referent were presented within four levels (within participants): 
Firstly, the 50%-present (P) and 50%-absent (A) trials could be either blocked (AAAPPP and PPPAAA) or not blocked (APAPAP and PAPAPA); secondly, the first encounter of a noun could be either an A trial (AAAPPP and APAPAP) or a P trial (PPPAAA and PAPAPA).

```{r}
load("other_exps/KoehneTrueswellGleitman2013orders.RData")

run_Koehne_exp <- function(model_name, par, ord) {
  source(paste0("models/",model_name,".R"))
  mp = model(par, ord)$matrix
  diag(mp[,1:8])/rowSums(mp[,1:8]) # 100% referents: ~60% perf
  diag(mp) = 0
  perf = c()
  for(w in 1:nrow(mp)) {
    perf[w] = mp[w,w+1]/sum(mp[w,1:9])
  }
  return(perf) 
}


evalSSE_Koehne <- function(model_name, par, orders, verbose=F) {
  df = tibble(Cond = names(Koehne), Human=rep(0.0, 4), Model=rep(0.0, 4), Model_sd=rep(0.0, 4))
  for(i in 1:length(Koehne)) {
    cond = names(Koehne)[i]
    # words needs to be an Nx1 matrix
    Koehne[[cond]]$train$words = matrix(Koehne[[cond]]$train$words, ncol=1)
    mperf = run_Koehne_exp(model_name, par, Koehne[[cond]]$train)
    df[i,]$Model = mean(mperf)
    df[i,]$Model_sd = sd(mperf)
    df[i,]$Human = Koehne[[cond]]$HumanAcc
  }
  if(verbose) print(df)
  return(sum((df$Human-df$Model)^2)/4)
}

# cv_group_fits[["kachergis"]]$pars
for(model_name in names(cv_group_fits)[1:6]) {
  sses = rep(NA, 5)
  for(i in 1:5) {
    pars = cv_group_fits[[model_name]]$pars[i,]
    sses[i] = evalSSE_Koehne("kachergis", pars, Koehne, verbose=F) # .017
  }
  print(paste(model_name, mean(sses)))
}
# problem with fazly model

# run_Koehne_exp("kachergis", c(0.1077372, 14.99996, 1), Koehne[["pppaaa"]]$train)


# the model predicts the same ordering as humans out of the box -- PPPAAA is best, AAAPPP is worst
# contradicts Koehne et al.'s interpretation: "This finding is inconsistent with a standard cross-situational
# account because all conditions should have been above chance independent of presentation order.

```


### Medina et al. 2011 
Medina et al. 2011 tasked adult participants with learning 12 nonce words presented across 60 training trials presenting only one word at a time.
The number of referents per training trial varied: High Informative (HI) trials showed two referents, while Low Informative (LI) trials showed five referents.
Unlike most cross-situational word learning experiments, each word depicted not a single referent, but instead corresponded to a category of five referents, each appearing  (e.g., "bosa" might appear with five pictures of bears on as many separate trials).
Different between-subjects conditions presented 12 HI trials (one per word) at the beginning, middle, or end of training, or not at all (HI-absent).

```{r, medina-exp}
load("other_exps/Medina2013orders.RData")

# graph performance per block (1-5)
get_block_perf <- function(modperf) {
  perf_inds = seq(12,60,12)
  perf = c()
  for(i in perf_inds) {
    perf = c(perf, mean(diag(modperf$traj[[i]]) / rowSums(modperf$traj[[i]])))
  }
  return(perf)
}

run_Medina_exp <- function(model_name, pars, plot=T) {
  source(paste0("models/",model_name,".R"))
  HI1 = model(pars, Medina[["HIfirst"]]$train)
  HIm = model(pars, Medina[["HImid"]]$train)
  HIl = model(pars, Medina[["HIlast"]]$train)
  HIabs = model(pars, Medina[["HIabsent"]]$train)

  mdat = data.frame(rbind(cbind(Condition="HI first", Vignette=1:5, Performance=get_block_perf(HI1)),
        cbind(Condition="HI middle", Vignette=1:5, Performance=get_block_perf(HIm)),
        cbind(Condition="HI last", Vignette=1:5, Performance=get_block_perf(HIl)),
        cbind(Condition="HI absent", Vignette=1:5, Performance=get_block_perf(HIabs)) ))
  mdat$Performance = as.numeric(as.character(mdat$Performance))
  mdat$Model = model_name
  
  if(plot) {
    p <- ggplot(mdat, aes(x=Vignette, y=Performance, group=Condition, shape=Condition, color=Condition)) + 
      geom_point() + geom_line() + theme_bw() + ylim(c(0,1.0))
    print(p)
  }
  return(mdat)
}

run_Medina_exp("kachergis", c(.126, 14.25, 1.0))
run_Medina_exp("fazly", c(0.4695008, 3.690278)) # Error in rowSums(modperf$traj[[i]]) : 'x' must be an array of at least two dimensions

```


### Yu, Zhong, and Fricker (2012)
Yu, Zhong, and Fricker (2012) pre-trained adult participants with three word-object associations and found higher performance on the other 15 words after this pre-training, demonstrating that knowing even a few word meanings can improve learning for other co-occurring words. 

```{r, pretraining-exp}
# orig4x4, but 3 pairs (which ones??) are pretrained: accuracy on those is .9187, 
# and accuracy on the other 15 is .5812 
# How variable is this effect based on selecting different 3 items for retraining?)
yuzhong_emp = c(.9187, .5812)

run_pretrain_experiment <- function(parms, n_pretrained=3, start_val=1) {
  voc_sz = 18
  start_matrix = matrix(0, voc_sz, voc_sz)
  pretrain_inds = sample(1:18, n_pretrained)
  other_inds = setdiff(1:18, pretrain_inds)
  #start_matrix[pretrain_inds, pretrain_inds] = .01 # irrelevant to seed off-diagonal or not
  diag(start_matrix)[pretrain_inds] = start_val
  mm = model(parms, orig4x4, start_matrix = start_matrix) 

  return(c(mean(mm$perf[pretrain_inds]), mean(mm$perf[other_inds])))
}

run_experiment_batch <- function(parms, n_pretrained=3, start_val=1, Nsim=100) {
  dat = data.frame()
  for(i in 1:Nsim) {
    dat = rbind(dat, round(run_pretrain_experiment(parms, n_pretrained=n_pretrained, start_val=start_val), 3))
  }
  names(dat) = c("pretrain", "other")
  return(dat)
}

mean(model(c(.1, 2, .92), orig4x4)$perf) # .45

pt3 = run_experiment_batch(c(.1, 2, .92), n_pretrained=3, start_val=1.6, Nsim=200)
colMeans(pt3) # pretrained = .89, other = .55

sum((colMeans(pt3) - yuzhong_emp)^2) # .002

pt6 = run_experiment_batch(c(.1, 2, .92), n_pretrained=6, start_val=1.6, Nsim=200)
colMeans(pt6) # pretrained = .89, other = .66

pt9 = run_experiment_batch(c(.1, 2, .92), n_pretrained=9, start_val=1.6, Nsim=200)
colMeans(pt9) # pretrained = .89, other = .76

```


### Suanda et al. (2014)
Suanda et al. (2014) varied contextual diversity in a cross-situational word learning study of 5- to 7-year-olds, presenting children with 8 to-be-learned words shown two per trial across a total of 16 trials.

```{r, suanda-exp}
# Suanda et al. 2014 - contextual diversity effects in children
# Low CD = .34 sd=.18
# Mod CD = .39 sd=.20
# Hi CD = .48 sd=.21
suanda_emp = c(.34, .39, .48)

loCD = read.table("Suanda2014-lowCD.txt", header=F, sep='\t') 
medCD = read.table("Suanda2014-medCD.txt", header=F, sep='\t') 
hiCD = read.table("Suanda2014-hiCD.txt", header=F, sep='\t') 

require(DEoptim)

# for specific order format: w o o o 
coocs <- function(ord) { 
  voc_sz = max(unlist(ord))
  m = matrix(0, nrow=voc_sz, ncol=voc_sz)
  for(t in 1:nrow(ord)) {
    tr = unlist(ord[t,])
    m[tr,tr] = m[tr,tr] + 1
  }
  return(m)
}

coocs(loCD)
coocs(medCD)
coocs(hiCD)

source("models/model.R")

evalSSE <- function(par, human, verbose=F) {
  mod_perf = c( mean(model(par, loCD)$perf),
                mean(model(par, medCD)$perf),
                mean(model(par, hiCD)$perf) )
  if(verbose) print(mod_perf)
  return(sum((human-mod_perf)^2))
}

Suanda_fit <- DEoptim(fn=evalSSE, human=suanda_emp, lower=c(.001, .1, .7), upper=c(2, 7, 1), 
                 DEoptim.control(reltol=.0001, steptol=50, itermax=200, trace=10))
evalSSE(c(0.044, 2.840, 0.999), suanda_emp, verbose=T) # SSE=.004
# 0.38 0.41 0.43

# adult params get high performance
evalSSE( c(0.490592, 6.927328, 0.802789), human, verbose=T) 
```


### Vlach & DeBrock (2017, 2019)
12 word-object pairs, 2 pairs/trial, 36 trials
6 pairs repeated, 6 spaced?

```{r, vlach-exp}
vlach_ord <- read.csv("other_exps/VlachDeBrock2017_2019.txt", header=F, sep='\t')

```


### Smith & Yu 2008
6 word-object pairs, 2 pairs/trial, 30 trials

```{r}
sy_ord <- read.csv("other_exps/smithYu2008-2x2-6w.txt", header=F, sep='\t')

```

