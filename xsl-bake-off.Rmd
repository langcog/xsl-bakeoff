---
title: "A large-scale comparison of several cross-situational word learning models"
author: "George Kachergis & Michael C. Frank"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning=F, message=F)
require(tidyverse)
require(ggplot2)
library(knitr)
library(kableExtra)
```

# Introduction

The goal of this study is to test the ability of a variety of cross-situational word learning models to account for a wide range of experimental data. 
Many models for cross-situational word learning have been proposed, and yet most of these models have only been tested on a few different experiments--often distinct ones, and only with one or two other models for comparison. 
Our goal is to scale up both the number of models evaluated and the number of experiment designs (and participants) modeled, recognizing that the plurality of models and the theories and intuitions they represent can only be winnowed down via rigorous comparison.
We will find best-fitting parameters for associative models, hypothesis-testing models, and a few hybrid models, and will quantify generalization on held-out data, as well as qualitatively in other experiments.
Finally, we will interpret the results, and what they entail for theories of cross-situational word learning.

### Models
* baseline count model (SSE=.620)
* propose-but-verify
* guess-and-test
* pursuit model
* Kachergis model (associative + sampling version)
    + strength bias model
    + uncertainty bias model
    + novelty bias model
* Fazly et al.
* "Bayesian" decay
* Tilles & Fontanari 2013 (ToDo: verify)
* Yurovsky & Frank 2015 (ToDo!!)


## The Data

```{r load-data}
load("data/combined_data.RData")

totSs = 0
totItems = 0
for(c in 1:length(combined_data)) { 
  totSs = totSs + combined_data[[c]]$Nsubj 
  totItems = totItems + length(combined_data[[c]]$HumanItemAcc)
}
```


The modeled data are average accuracies from `r totItems` word-object pairs in `r length(combined_data)` experimental conditions, in which a total of `r totSs` subjects participated.
Most of these data have been previously published: 

We will first optimize each model's parameters with 5-fold cross-validation, leaving out data from 20% of the conditions to test generalization from optimizing to the remaining 80%.
We will then optimize each model's parameters to the entire dataset, and use these parameters to test generalization to a selection of other experiments from the literature for which we do not have item-level accuracy.


```{r, load-trajectory-data, include=F}
# individual-level subject data for 18 conditions (from Kachergis & Yu 2013)
#load("data/asym_master.RData") 
#asym <- raw %>% group_by(Condition, CondNum, Subject) %>%
#  summarise(perf = mean(Correct)) %>%
#  group_by(Condition, CondNum) %>% summarise(Nsubj=n()) 
# CondNums with missing names: 
# 217 - 2 words x 4 objects, 54 trials
# 218 - 3 words x 4 objects, 36 trials
# 220 - 3 words x 4 objects, 54 trials
# sum(asym$Nsubj) # 556 subjects

#load("data/asymmetric_conditions.RData") 
#load("data/master_orders.RData") # trial orders for ME (filt) and FreqCD papers

# multi-block experiment: test generalization?
#load("data/priming_all_trajectory.RData") 
# 86 subjects on 4 blocks of equal frequency 4pairs/trial, or 3,6,9x freq 3 or 4 pairs per trial
#asym <- raw %>% group_by(Condition, CondNum, Subject) %>%
#  summarise(perf = mean(Correct)) %>%
#  group_by(Condition, CondNum) %>% summarise(Nsubj=n())
```

Each condition consists of an ordered list of training trials consisting of 1-4 words and 2-4 objects per trial.
We also require a test function--often presenting each word a single time along *m*AFC of objects, where *m* is the number of objects seen during the training, but sometimes a subset of the objects are tested.

## Model Descriptions

We fit the following associative models: Fazly [@fazly2010], Kachergis [@kachergis2012], ... and a baseline co-occurrence counting model.
We also fit two hypothesis-testing models: guess-and-test [@trueswell2011,@blythe2010] and propose-but-verify [@trueswell2013].
Finally, we fit two hybrid models, that store multiple, graded hypotheses--but only a subset of all possible assoociations: pursuit [@stevens2017], and a stochastic version of the Kachergis model.
These models and their free parameters are described below.

### Propose-but-verify
In the propose-but-verify hypothesis testing model @trueswell2013, one of the presented referents is selected at random for any word heard that has no remembered referent. 
The next time that word occurs, the previously-proposed referent is remembered with probability $\alpha$, a free parameter. 
If the remembered referent is verified to be present, the future probability of recalling the word is increased by $\epsilon$ (another free parameter). 
If the remembered referent is not present, the old hypothesis is assumed to be forgotten and a new proposal is selected from the available referents.
This model implements trial-level mutual exclusivity by selecting new proposals only from among the referents that are not yet part of a hypothesis.

### Guess-and-test
The guess-and-test hypothesis testing model is based on the description given by @medina2011 of a one-shot (i.e. "fast mapping") learning, which posits that "i) learners hypothesize a single meaning based on their first encounter with a word, ii) learners neither weight nor even store back-up alternative meanings, and iii) on later encounters, learners attempt to retrieve this hypothesis from memory and test it against a new context, updating it only if it is disconfirmed."
In summary, guess-and-test learners do *not* reach a final hypothesis by comparing multiple episodic memories of prior contexts or multiple semantic hypotheses.
We give this model two free parameters: a probability of successful encoding ($s$, hypothesis formation), and a probability $f$ of forgetting a hypothesis at retrieval. 
This model is quite similar to the guess-and-test model formally analyzed by @blythe2010 to determine the theoretical long-term efficacy of cross-situational word learning.

```{r, model-list}
# need to be run >100 times per given parameter values
stochastic_models = list.files("models/stochastic/")
# assoc models need to be run only once per given parameter values
assoc_models = list.files("models/") 
assoc_models = assoc_models[assoc_models!="stochastic"]

```

```{r, load-fits}
#source("fitting_functions.R")

# each model jointly fit to all conditions
load("fits/group_fits.Rdata")
#for(m in names(group_fits)) { 
#  print(paste(m, round(group_fits[[m]]$optim$bestval, 2)))
#}
# each model separately fit to each condition (should try cross-validation approach?)

load("fits/cond_fits.Rdata")

#load("fits/individual_fits.Rdata")
# not all conditions have subject-level data (but I think I can track it down)
# moreover, some subjects participated in multiple conditions, but I'm not sure 
# I can trace that for all of the conditions (certainly for the exps I ran -- ~25 conditions)

load("fits/cv_group_fits.Rdata")
cvg <- tibble()
for(m in names(cv_group_fits)) {
  cvg <- rbind(cvg, cv_group_fits[[m]]$testdf)
}

cvg_fit_tab <- cvg %>% group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)


```

## Model Fitting Procedure

We will fit each model with five-fold cross-validation method, leaving out 9 of the `r length(combined_data)` conditions in each of 4 folds, and 8 in the final.
The 2 or 3 free parameters for each model were optimized using differential evolution [@DEoptim], a global optimization algorithm that requires no assumptions about a differentiable fitness landscape (which may not be met here) and works well when there may be many local minima.

# Results


## Conditions-left-out Cross-validated Group Fits


```{r}
kable(cvg_fit_tab, digits=3, caption="Group model fits.")
```

## Group-level Model Fits

```{r, summarize-group-fits}
group_fit_tab <- gfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)

kable(group_fit_tab, digits=3, caption="Group model fits.") 
```


Plotted by condition means.

```{r, plot-group-fits-avg, fig.width=8}
# look at condition average fits
ccondm <- cfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#ccondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)


gcondm <- gfd %>% group_by(Model, Condition, Nsubj) %>%
  summarise(HumanPerf=mean(HumanPerf), 
            ModelPerf = mean(ModelPerf))

# condition-level (above is item-level)
#gcondm %>% group_by(Model) %>% 
#  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
#            r = cor(ModelPerf, HumanPerf)) %>% 
#  arrange(SSE)

gcondm %>%
  ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition, size=Nsubj)) + 
  geom_point(alpha=.7) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")
  #theme(legend.position="bottom")
```

Plotted by individual items.

```{r, plot-group-fits, fig.width=8, caption="Model vs. human performance by model and condition for group model fits."}
gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition)) + 
  geom_point(alpha=.8) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")

#gfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Model, color=Model)) + 
#  geom_point() + facet_wrap(vars(Condition)) + theme_bw() + geom_smooth(method = "lm")
```

## Per-Condition Model Fits
(not done yet for sampling models, and we may not even want to do this except for a version with cross-validation)

```{r, summarize-condition-fits}
cond_fit_tab <- cfd %>% filter(!is.na(HumanPerf)) %>%
  group_by(Model) %>% 
  summarise(SSE = sum((ModelPerf-HumanPerf)^2),
            r = cor(ModelPerf, HumanPerf)) %>%
  arrange(SSE)

kable(cond_fit_tab, digits=3, caption="By-condition model fits.") 
```

```{r, plot-cond-fits, fig.width=8, caption="Model vs. human performance by model and condition for per-condition model fits."}
cfd %>% ggplot(aes(x=ModelPerf, y=HumanPerf, group=Condition, color=Condition)) + 
  geom_point(alpha=.8) + facet_wrap(vars(Model)) + theme_bw() +
  geom_abline(intercept=0, slope=1, linetype="dashed")
```


## Generalization Experiments

Using the parameters optimized to each model for all of the conditions, we simulate model performance for a selection of published experiments and compare to participants' group-level performance for these conditions. 
The first three studies are of adult participants, and include two experiments that have been interpreted as supporting hypothesis-testing theories of word learning.


### Koehne, Trueswell & Gleitman (2013)
For example, Koehne, Trueswell & Gleitman (2013), in which each of 16 novel nouns was assigned two meanings with different co-occurrence frequencies: 
One referent was present whenever the noun was present (six times, 100% referent), the other referent was present in only half of the cases the noun was (three times, 50% referent). All other objects co-occurred only once with a noun (17%). 
Training trial order was manipulated: including and excluding the 50% referent were presented within four levels (within participants): 
Firstly, the 50%-present (P) and 50%-absent (A) trials could be either blocked (AAAPPP and PPPAAA) or not blocked (APAPAP and PAPAPA); secondly, the first encounter of a noun could be either an A trial (AAAPPP and APAPAP) or a P trial (PPPAAA and PAPAPA).

```{r}
load("other_exps/KoehneTrueswellGleitman2013orders.RData")

# ToDo: modify to work for stochastic models, too
run_Koehne_exp <- function(model_name, par, ord) {
  source(paste0("models/",model_name,".R"))
  mp = model(par, ord)$matrix
  diag(mp[,1:8])/rowSums(mp[,1:8]) # 100% referents: ~60% perf
  diag(mp) = 0
  perf = c()
  for(w in 1:nrow(mp)) {
    perf[w] = mp[w,w+1]/sum(mp[w,1:9])
  }
  return(perf) 
}


evalSSE_Koehne <- function(model_name, par, orders, verbose=F) {
  df = tibble(Cond = names(Koehne), Human=rep(0.0, 4), Model=rep(0.0, 4), Model_sd=rep(0.0, 4))
  for(i in 1:length(Koehne)) {
    cond = names(Koehne)[i]
    # words needs to be an Nx1 matrix
    Koehne[[cond]]$train$words = matrix(Koehne[[cond]]$train$words, ncol=1)
    mperf = run_Koehne_exp(model_name, par, Koehne[[cond]]$train)
    df[i,]$Model = mean(mperf)
    df[i,]$Model_sd = sd(mperf)
    df[i,]$Human = Koehne[[cond]]$HumanAcc
  }
  if(verbose) print(df)
  return(sum((df$Human-df$Model)^2)/4)
}

gen_exps = tibble(exp=character(), model=character(), SSE=numeric())
for(model_name in names(group_fits)[1:6]) {
  pars = group_fits[[model_name]]$optim$bestmem
  sse = evalSSE_Koehne(model_name, pars, Koehne, verbose=F) # 
  gen_exps = bind_rows(gen_exps, tibble(exp="Koehne et al.", model=model_name, SSE=sse))
}

#run_Koehne_exp("kachergis", group_fits$kachergis$optim$bestmem, Koehne[["pppaaa"]]$train)
# Error in 1:nrow(ord$words) : argument of length 0

# the model predicts the same ordering as humans out of the box -- PPPAAA is best, AAAPPP is worst
# contradicts Koehne et al.'s interpretation: "This finding is inconsistent with a standard cross-situational
# account because all conditions should have been above chance independent of presentation order.

```


### Medina et al. 2011 
Medina et al. 2011 tasked adult participants with learning 12 nonce words presented across 60 training trials presenting only one word at a time.
The number of referents per training trial varied: High Informative (HI) trials showed two referents, while Low Informative (LI) trials showed five referents.
Unlike most cross-situational word learning experiments, each word depicted not a single referent, but instead corresponded to a category of five referents, each appearing  (e.g., "bosa" might appear with five pictures of bears on as many separate trials).
Different between-subjects conditions presented 12 HI trials (one per word) at the beginning, middle, or end of training, or not at all (HI-absent).

```{r, medina-exp}
load("other_exps/Medina2013orders.RData")
# HIfirst: 66% accuracy on vignette 1; 41% accuracy by the fifth vignette; offer this guess as “the meaning of the word” at experiment’s end (37% accuracy on the Final Guess)
# they don't list the means per cond/vignette, except in Fig2. 
# I tried to read off the performance from the Figure...
Medina[["HIfirst"]]$HumAcc = c(.66, .33, .39, .46, .41) # final: .37
Medina[["HImid"]]$HumAcc = c(.14, .14, .39, .30, .16) # final: .09
Medina[["HIlast"]]$HumAcc = c()
Medina[["HIabsent"]]$HumAcc = c()
  
# graph performance per block (1-5)
get_block_perf <- function(modperf) {
  perf_inds = seq(12,60,12)
  perf = c()
  for(i in perf_inds) {
    perf = c(perf, mean(diag(modperf$traj[[i]]) / rowSums(modperf$traj[[i]])))
  }
  return(perf)
}

run_Medina_exp <- function(model_name, pars, plot=T) {
  source(paste0("models/",model_name,".R"))
  HI1 = model(pars, Medina[["HIfirst"]]$train)
  HIm = model(pars, Medina[["HImid"]]$train)
  HIl = model(pars, Medina[["HIlast"]]$train)
  HIabs = model(pars, Medina[["HIabsent"]]$train)

  mdat = data.frame(rbind(cbind(Condition="HI first", Vignette=1:5, Performance=get_block_perf(HI1)),
        cbind(Condition="HI middle", Vignette=1:5, Performance=get_block_perf(HIm)),
        cbind(Condition="HI last", Vignette=1:5, Performance=get_block_perf(HIl)),
        cbind(Condition="HI absent", Vignette=1:5, Performance=get_block_perf(HIabs)) ))
  mdat$Performance = as.numeric(as.character(mdat$Performance))
  mdat$Model = model_name
  
  if(plot) {
    p <- ggplot(mdat, aes(x=Vignette, y=Performance, group=Condition, shape=Condition, color=Condition)) + 
      geom_point() + geom_line() + theme_bw() + ylim(c(0,1.0))
    print(p)
  }
  return(mdat)
}

run_Medina_exp("kachergis", c(.126, 14.25, 1.0))
#run_Medina_exp("fazly", c(0.4695008, 3.690278)) # Error in rowSums(modperf$traj[[i]]) : 'x' must be an array of at least two dimensions

# ToDo: write evalSSE_Medina()
```


### Yu, Zhong, and Fricker (2012)
Yu, Zhong, and Fricker (2012) pre-trained adult participants with three word-object associations and found higher performance on the other 15 words after this pre-training, demonstrating that knowing even a few word meanings can improve learning for other co-occurring words. 

```{r, pretraining-exp}
# orig4x4, but 3 pairs (which ones??) are pretrained: accuracy on those is .9187, 
# and accuracy on the other 15 is .5812 
# How variable is this effect based on selecting different 3 items for retraining?)
yuzhong_emp = c(.9187, .5812)
yuzhong_ord = combined_data[["orig_4x4"]]$train

# pre-train a random subset of n_pretrained word-object pairs
run_pretrain_experiment <- function(parms, n_pretrained=3, start_val=1) {
  voc_sz = 18
  start_matrix = matrix(0, voc_sz, voc_sz)
  pretrain_inds = sample(1:18, n_pretrained)
  other_inds = setdiff(1:18, pretrain_inds)
  #start_matrix[pretrain_inds, pretrain_inds] = .01 # irrelevant to seed off-diagonal or not
  diag(start_matrix)[pretrain_inds] = start_val
  mm = model(parms, yuzhong_ord, start_matrix = start_matrix) 

  return(c(mean(mm$perf[pretrain_inds]), mean(mm$perf[other_inds])))
}

# run Nsim pretraining experiments (with different random pretrained items)
run_pretrain_experiment_batch <- function(model_name, parms, n_pretrained=3, start_val=1, Nsim=100) {
  source(paste0("models/",model_name,".R"))
  dat = data.frame()
  for(i in 1:Nsim) {
    dat = rbind(dat, round(run_pretrain_experiment(parms, n_pretrained=n_pretrained, start_val=start_val), 3))
  }
  names(dat) = c("pretrain", "other")
  return(dat)
}

pt3 = run_pretrain_experiment_batch("kachergis", c(.1, 2, .92), n_pretrained=3, start_val=1.6, Nsim=200)
colMeans(pt3) # pretrained = .89, other = .55

sum((colMeans(pt3) - yuzhong_emp)^2) # .002

pt6 = run_pretrain_experiment_batch("kachergis", c(.1, 2, .92), n_pretrained=6, start_val=1.6, Nsim=200)
colMeans(pt6) # pretrained = .89, other = .66

pt9 = run_pretrain_experiment_batch("kachergis", c(.1, 2, .92), n_pretrained=9, start_val=1.6, Nsim=200)
colMeans(pt9) # pretrained = .89, other = .76

```


### Suanda et al. (2014)
Suanda et al. (2014) varied contextual diversity in a cross-situational word learning study of 5- to 7-year-olds, presenting children with 8 to-be-learned words shown two per trial across a total of 16 trials.
...2AFC?
```{r, suanda-exp}
# Suanda et al. 2014 - contextual diversity effects in children
load("other_exps/Suanda2014orders.RData") # Suanda orders and HumanAcc

# Suanda[["Low CD"]]: each pair appears 3 times with another pair and once with a 2nd pair
# Medium CD: each pair appears once with two other pairs and twice with another pair
# High CD: each pair appears with 4 other pairs

evalSSE_orders <- function(model_name, par, orders, verbose=F) {
  df = tibble(Cond = character(), Human=numeric(), Model=numeric(), Model_sd=numeric())
  for(i in 1:length(orders)) {
    cond = names(orders)[i]
    mperf = model(par, orders[[cond]]$train)$perf
    df = bind_rows(df, tibble(cond=cond, Model=mean(mperf), Model_sd=sd(mperf), 
                              Human=orders[[cond]]$HumanAcc))
  }
  if(verbose) print(df)
  return(sum((df$Human-df$Model)^2)/length(orders))
}

for(model_name in names(group_fits)[1]) { # [1:6]
  pars = group_fits[[model_name]]$optim$bestmem
  sse = evalSSE_orders(model_name, pars, Suanda, verbose=F) # 
  gen_exps = bind_rows(gen_exps, tibble(exp="Suanda et al.", model=model_name, SSE=sse))
}
# fazly: Error in if (min(p) < 0 || sum(p) <= 0) return(NA) : missing value where TRUE/FALSE needed
```


### Vlach & DeBrock (2017, 2019)
12 word-object pairs, 2 pairs/trial, 36 trials
6 pairs massed, 6 interleaved
2AFC testing

```{r, vlach-exp}
vlach_ord <- read.csv("other_exps/VlachDeBrock2017_2019.txt", header=F, sep='\t')
# 2017 paper had immediate testing: 2–3 year-olds (61% correct responses), 4–5 year-olds (67% correct)
#  - no diff between interleaved/massed with the immediate test
# 2019 paper introduced a 5-minute delay between learning and test phases (children aged 25-58 mos), M=.57
#  - sig. higher performance on the interleaved word-object mappings

mAFC_test <- function(mat, m) {
  perf = rep(0, nrow(mat))
  for(i in 1:nrow(mat)) {
    # sample m-1 incorrect referents
    alts = sample(setdiff(1:ncol(mat), i), m-1)
    perf[i] = mat[i,i] / sum(mat[i,c(alts, i)])
  }
  return(perf)
}

vlach = list(train = list(words = vlach_ord, objs = vlach_ord))

massed = c(1, 8, 9, 10, 11, 12)
spaced = setdiff(1:12, massed)
# 2AFC test, overall M=6.81 words learned (better with age: only 47-58 mos / 50-70 mos groups above chance
# Exp1: massed M=3.17, interleaved M=3.64 
# Exp2: massed M=3.00, interleaved M=3.48

mp = model(c(.003, 2, .97), vlach$train)
mperf = mAFC_test(mp$matrix, 2)
massed_perf = sum(mperf[massed]) 
spaced_perf = sum(mperf[spaced]) 
```


### Smith & Yu 2008 / Yu & Smith (2011)
6 word-object pairs, 2 pairs/trial, 30 trials

```{r}
sy_ord <- read.csv("other_exps/smithYu2008-2x2-6w.txt", header=F, sep='\t')
smith_yu = list(train=list(words=sy_ord, objs=sy_ord))


# with adult parameters, learns 5.76 words
#mp = model(group_fits$kachergis$optim$bestmem, smith_yu$train)

#mp = model(c(.0015, 2, .97), smith_yu$train)
#sum(mAFC_test(mp$matrix, 2)) # 3.57 words learned

#mp = model(c(.003, 1, .97), smith_yu$train)
#sum(mAFC_test(mp$matrix, 2)) # 4.00 words learned
```

